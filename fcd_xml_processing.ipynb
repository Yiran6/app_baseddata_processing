{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import Counter\n",
    "from geopandas import GeoDataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"G:/My Drive/2021/Bias/sumo_simulation/appsim\"\n",
    "os.chdir(path)\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other method \n",
    "#considering taz as a point and construct a network\n",
    "#possible resource\n",
    "#https://github.com/sharifulgeo/ESRI/tree/master/Connect_Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get taz data\n",
    "taz_tree = ET.parse('Taz_bigger_Seattle_all_fordtagg.add.xml')\n",
    "taz_root = taz_tree.getroot()\n",
    "\n",
    "#for i in root.iter('timestep'):\n",
    "#    print(i.attrib)\n",
    "##output:{'time': '0.00'} {'time': '300.00'}\n",
    "#get net data\n",
    "net_tree = ET.parse('test_2_traffic_sig_revise_5.net.xml')\n",
    "net_root = net_tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcd_output.xml\n",
      "fcd_output_5.xml\n",
      "fcd_output_5_30sec.xml\n",
      "fcd_output_10per_all_30sec.xml\n",
      "fcd_output_10per_ht_30sec.xml\n",
      "fcd_output_10per_all_1sec.xml\n",
      "fcd_output_10per_ht_1sec.xml\n",
      "fcd_output_10per_all_30sec_dtSea.xml\n",
      "fcd_output_10per_ht_30sec_dtSea.xml\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir():\n",
    "    if 'fcd' in i and 'xml' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the fcd output to check the data\n",
    "#fcd_file = 'fcd_output_10per_all_30sec.xml'\n",
    "#fcd_file = 'fcd_output_10per_ht_30sec.xml'\n",
    "#fcd_file = 'fcd_output_10per_all_30sec_dtSea.xml'\n",
    "fcd_file = 'fcd_output_10per_ht_30sec_dtSea.xml'\n",
    "\n",
    "tree = ET.parse(fcd_file)\n",
    "root = tree.getroot()\n",
    "#for i in root.iter('timestep'):\n",
    "#    print(i.attrib)\n",
    "##output:{'time': '0.00'} {'time': '300.00'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing TAZs processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges in taz {}\n",
      "edge with tazs identified: 7220\n",
      "edge w.o tazs identified: 9218\n",
      "9218\n",
      "9218\n",
      "1704\n"
     ]
    }
   ],
   "source": [
    "#defined packages\n",
    "def assign_val(missing_edge_connection, target_edge, connect_edge):\n",
    "    if target_edge not in missing_edge_connection:\n",
    "        missing_edge_connection[target_edge] = []\n",
    "        missing_edge_connection[target_edge].append(connect_edge)\n",
    "    else:\n",
    "        if connect_edge not in missing_edge_connection[target_edge]:\n",
    "            missing_edge_connection[target_edge].append(connect_edge)\n",
    "    return(missing_edge_connection)\n",
    "\n",
    "def convert2taz(connect_edges):\n",
    "    connect_taz = []\n",
    "    for edge in connect_edges:\n",
    "        if edge in edge_taz_dict:\n",
    "            connect_taz.append(edge_taz_dict[edge])\n",
    "    include_taz = np.unique(connect_taz)\n",
    "    taz_ct = []\n",
    "    for taz in include_taz:\n",
    "        taz_ct.append(connect_taz.count(taz))\n",
    "    try:\n",
    "        return(include_taz[taz_ct.index(max(taz_ct))])\n",
    "    except:\n",
    "        return(-1)\n",
    "            \n",
    "def sep_str(lane_str):\n",
    "    if len(lane_str) != 0:\n",
    "        return(lane_str.split(' '))\n",
    "    else:\n",
    "        return('1')\n",
    "    \n",
    "def get_edge(int_inc_lane, edge_net_lst):\n",
    "    edge_cur = []\n",
    "    for l in int_inc_lane:\n",
    "        if l != '1':\n",
    "            if l[-2] == '_':\n",
    "                if l[:-2] in edge_net_lst:\n",
    "                    edge_cur.append(l[:-2])\n",
    "            if l[-3] == '_':\n",
    "                if l[:-3] in edge_net_lst:\n",
    "                    if l[:-3] not in edge_cur:\n",
    "                        edge_cur.append(l[:-3])\n",
    "    return(edge_cur)\n",
    "\n",
    "def get_taz(edge, edge_taz_dict):\n",
    "    try:\n",
    "        return(edge_taz_dict[edge])\n",
    "    except:\n",
    "        return(0)\n",
    "    \n",
    "def assign_edge_val(edge_cur, junctionid, edge_taz_dict):\n",
    "    edge_val = []\n",
    "    for edge in edge_cur:\n",
    "        val = get_taz(edge, edge_taz_dict)\n",
    "        if val == 0:\n",
    "            pass\n",
    "        else:\n",
    "            edge_val.append(val)\n",
    "    edge_val.append(junctionid)\n",
    "    return(edge_val)   \n",
    "  \n",
    "    \n",
    "#get edge id in tazs\n",
    "edge_taz_dict = {}\n",
    "edges_in2tazs = {}\n",
    "for tazs in taz_root.iter('taz'):\n",
    "    taz_id_cur = tazs.attrib['id']\n",
    "    for edge in tazs.findall('tazSource'):\n",
    "        edge_id_cur = edge.attrib['id']\n",
    "        if edge_id_cur not in edge_taz_dict:\n",
    "            edge_taz_dict[edge_id_cur] = taz_id_cur\n",
    "        else:\n",
    "            #print('warning, edges appear in more than one taz')\n",
    "            edges_in2tazs[edge_id_cur] = []\n",
    "            edges_in2tazs[edge_id_cur].append(edge_taz_dict[edge_id_cur])\n",
    "            edges_in2tazs[edge_id_cur].append(taz_id_cur)\n",
    "            \n",
    "print('edges in taz', edges_in2tazs)\n",
    "\n",
    "#get fcd data\n",
    "#check edges in the data\n",
    "edge_lst = []\n",
    "lane_lst = []\n",
    "\n",
    "for t in root.findall('timestep'):\n",
    "    #get lane id from vehicle\n",
    "    for veh in t.findall('vehicle'):\n",
    "        #print(veh.attrib['id'])\n",
    "        lane_id_cur = veh.attrib['lane']\n",
    "        #if '_' in lane_id_cur:\n",
    "        #    lane_id_cur = ExtractEdgeFromLane(lane_id_cur)\n",
    "        if lane_id_cur not in lane_lst:\n",
    "            lane_lst.append(lane_id_cur)\n",
    "    for per in t.findall('person'):\n",
    "        edge_id_cur = per.attrib['edge']\n",
    "        if edge_id_cur not in edge_lst:\n",
    "            edge_lst.append(edge_id_cur)\n",
    "            \n",
    "#convert lane to edge from net file\n",
    "lane_dict = {}\n",
    "edge_net_lst = [] #get all edges from the network\n",
    "for edge in net_root.findall('edge'):\n",
    "    edge_id = edge.attrib['id']\n",
    "    if edge_id not in edge_net_lst:\n",
    "        edge_net_lst.append(edge_id)   \n",
    "    for lane in edge.findall('lane'):\n",
    "        lane_id = lane.attrib['id']\n",
    "       # print(lane_id)\n",
    "        if lane_id in lane_lst and lane_id not in lane_dict:\n",
    "            lane_dict[lane_id] = edge_id\n",
    "#print(lane_dict)\n",
    "\n",
    "for edge in list(lane_dict.values()):\n",
    "    if edge not in edge_lst:\n",
    "        edge_lst.append(edge)\n",
    "        \n",
    "\n",
    "edges_missing_tazs = []\n",
    "edges_with_tazs = list(edge_taz_dict.keys())\n",
    "for edges in edge_lst:\n",
    "    if edges not in edges_with_tazs and edges not in edges_missing_tazs:\n",
    "        edges_missing_tazs.append(edges)\n",
    "        \n",
    "missings_taz_veh = []\n",
    "missings_taz_ped_pub = []\n",
    "for edges in edges_missing_tazs:\n",
    "    if \":\" in edges: \n",
    "        missings_taz_ped_pub.append(edges)\n",
    "    else:\n",
    "        missings_taz_veh.append(edges)\n",
    "        \n",
    "print('edge with tazs identified:', len(edges_with_tazs))\n",
    "print('edge w.o tazs identified:', len(edges_missing_tazs))\n",
    "\n",
    "#assign taz to missing edges\n",
    "missing_edge_connection = {}\n",
    "            \n",
    "for c in net_root.findall('connection'):\n",
    "    edge1 = c.attrib['from']\n",
    "    edge2 = c.attrib['to']\n",
    "    #print(edge1, edge2)\n",
    "    \n",
    "    if edge1 in edges_missing_tazs:\n",
    "        #print(edge1)\n",
    "        missing_edge_connection = assign_val(missing_edge_connection,\n",
    "                                             edge1, edge2)\n",
    "    elif edge2 in edges_missing_tazs:\n",
    "        #print(edge2)\n",
    "        missing_edge_connection = assign_val(missing_edge_connection,\n",
    "                                            edge2, edge1)\n",
    "        \n",
    "#print(len(list(missing_edge_connection.keys())))\n",
    "#print(len(edges_missing_tazs))\n",
    "#previous return = 6329 (both)\n",
    "\n",
    "print(len(list(missing_edge_connection.keys())))\n",
    "print(len(edges_missing_tazs))\n",
    "#previous return = 3203 (for 30 sec data)\n",
    "\n",
    "#for edges in missing_edge_connection:\n",
    "#print('test', convert2taz(missing_edge_connection['165086027#0']))\n",
    "\n",
    "missing_edge_taz = {}\n",
    "for edge in missing_edge_connection:\n",
    "    missing_edge_taz[edge] = convert2taz(missing_edge_connection[edge])\n",
    "    \n",
    "#get edge with no taz mapped\n",
    "edges_ = []\n",
    "index_ = 0\n",
    "for i in list(missing_edge_taz.values()):\n",
    "    if i == -1:\n",
    "        edges_.append(list(missing_edge_taz.keys())[index_])\n",
    "    index_ += 1\n",
    "    \n",
    "for edge in missing_edge_taz:\n",
    "    taz_val = missing_edge_taz[edge]\n",
    "    if taz_val != -1:\n",
    "        if edge not in edge_taz_dict:\n",
    "            edge_taz_dict[edge] = taz_val\n",
    "        else:\n",
    "            print('edge in taz already', edge)   \n",
    "            \n",
    "#check '_' in lanes and edges\n",
    "for i in edge_net_lst:\n",
    "    if i[-2] == '_':\n",
    "        pass\n",
    "    if i[-3] == '_':\n",
    "        pass\n",
    "    if i[-4] == '_':\n",
    "        print(i)\n",
    "\n",
    "\n",
    "#get junction\n",
    "missing_edge_junc = {}\n",
    "#index_ = 0\n",
    "for junc in net_root.findall('junction'):\n",
    "    #print(index_)\n",
    "    int_lane = junc.attrib['intLanes']\n",
    "    inc_lane = junc.attrib['incLanes']\n",
    "    junctionid = junc.attrib['id']\n",
    "\n",
    "    int_inc_lane = []\n",
    "    int_inc_lane.extend(sep_str(int_lane))\n",
    "    int_inc_lane.extend(sep_str(inc_lane))\n",
    "    #print(int_inc_lane)\n",
    "    if len(int_inc_lane) != 0:\n",
    "        edge_cur = get_edge(int_inc_lane, edge_net_lst)\n",
    "    for edge in edge_cur:\n",
    "        if edge in edges_:\n",
    "            if edge not in missing_edge_junc:\n",
    "                missing_edge_junc[edge] = assign_edge_val(edge_cur, \n",
    "                                                          junctionid, \n",
    "                                                          edge_taz_dict)            \n",
    "\n",
    "print(len(missing_edge_junc))\n",
    "selected_junc = list(missing_edge_junc.values())\n",
    "\n",
    "index_ = 0\n",
    "for edge in missing_edge_junc:\n",
    "    tazs = missing_edge_junc[edge][:-1]\n",
    "    #print(tazs)\n",
    "    taz_ct = []\n",
    "    unique_taz = np.unique(tazs)\n",
    "    #print(unique_taz)\n",
    "    for taz in unique_taz:\n",
    "        taz_ct.append(tazs.count(taz))\n",
    "    #print(taz_ct)\n",
    "    try:\n",
    "        taz_ = unique_taz[taz_ct.index(max(taz_ct))]\n",
    "    except:\n",
    "        taz_ = -1\n",
    "    #print(taz_)\n",
    "    if edge not in edge_taz_dict and taz_!=-1:\n",
    "        edge_taz_dict[edge] = taz_\n",
    "    elif taz_ == -1:\n",
    "        print(edge)\n",
    "#through the check, we assign 629 to the missing edges\n",
    "#edge_taz_dict[':53217375_5'] = '629'\n",
    "\n",
    "#missing_edge_junc[':53147426_0']\n",
    "#edge_taz_dict[':cluster_gneJ449_gneJ451_1']\n",
    "#edge_taz_dict[lane_dict['460423550#0_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fcd data processing (ground truth with taz assigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data output done\n"
     ]
    }
   ],
   "source": [
    "#get total taz in sumo network\n",
    "taz_lst = []\n",
    "for taz in taz_root.findall('taz'):\n",
    "    taz_lst.append(taz.attrib['id'])\n",
    "    \n",
    "#def get_dt(fcd_outputpath, data_path, lane_dict, edge_taz_dict):\n",
    "    #use the fcd output to check the data\n",
    "#using 30 sec as an example\n",
    "#fcd_outputpath = 'fcd_output_10per_all_30sec.xml'\n",
    "#fcd_outputpath = 'fcd_output_10per_ht_30sec.xml'\n",
    "#fcd_file\n",
    "\n",
    "\n",
    "write_line = ''\n",
    "\n",
    "#tree = ET.parse(fcd_outputpath)\n",
    "#root = tree.getroot()\n",
    "with open (fcd_file[:-3]+'csv', 'w') as f:\n",
    "    for t in root.findall('timestep'):\n",
    "        time_cur = t.attrib['time']\n",
    "        #print(time_cur)\n",
    "        #get lane id from vehicle\n",
    "        for veh in t.findall('vehicle'):\n",
    "            vehid = veh.attrib['id']\n",
    "            #print(vehid)\n",
    "            lane_id_cur = veh.attrib['lane']\n",
    "            taz_id = edge_taz_dict[lane_dict[lane_id_cur]]\n",
    "            write_line = vehid+','+taz_id+','+time_cur+'\\n'\n",
    "            f.write(write_line)\n",
    "        try:    \n",
    "            for per in t.findall('person'):\n",
    "                perid = per.attrib['id']\n",
    "                #print(perid)\n",
    "                edge_id_cur = per.attrib['edge']\n",
    "                taz_id = edge_taz_dict[edge_id_cur]\n",
    "                write_line = perid+','+taz_id+','+time_cur+'\\n'\n",
    "                f.write(write_line)\n",
    "        except: \n",
    "            pass\n",
    "f.close()\n",
    "print('data output done')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the fcd output to check the data\n",
    "fcd_file = 'fcd_output_10per_all_1sec.xml'\n",
    "#fcd_file = 'fcd_output_10per_ht_1sec.xml'\n",
    "tree = ET.parse(fcd_file)\n",
    "root = tree.getroot()\n",
    "#for i in root.iter('timestep'):\n",
    "#    print(i.attrib)\n",
    "##output:{'time': '0.00'} {'time': '300.00'}\n",
    "\n",
    "#defined packages\n",
    "def assign_val(missing_edge_connection, target_edge, connect_edge):\n",
    "    if target_edge not in missing_edge_connection:\n",
    "        missing_edge_connection[target_edge] = []\n",
    "        missing_edge_connection[target_edge].append(connect_edge)\n",
    "    else:\n",
    "        if connect_edge not in missing_edge_connection[target_edge]:\n",
    "            missing_edge_connection[target_edge].append(connect_edge)\n",
    "    return(missing_edge_connection)\n",
    "\n",
    "def convert2taz(connect_edges):\n",
    "    connect_taz = []\n",
    "    for edge in connect_edges:\n",
    "        if edge in edge_taz_dict:\n",
    "            connect_taz.append(edge_taz_dict[edge])\n",
    "    include_taz = np.unique(connect_taz)\n",
    "    taz_ct = []\n",
    "    for taz in include_taz:\n",
    "        taz_ct.append(connect_taz.count(taz))\n",
    "    try:\n",
    "        return(include_taz[taz_ct.index(max(taz_ct))])\n",
    "    except:\n",
    "        return(-1)\n",
    "            \n",
    "def sep_str(lane_str):\n",
    "    if len(lane_str) != 0:\n",
    "        return(lane_str.split(' '))\n",
    "    else:\n",
    "        return('1')\n",
    "    \n",
    "def get_edge(int_inc_lane, edge_net_lst):\n",
    "    edge_cur = []\n",
    "    for l in int_inc_lane:\n",
    "        if l != '1':\n",
    "            if l[-2] == '_':\n",
    "                if l[:-2] in edge_net_lst:\n",
    "                    edge_cur.append(l[:-2])\n",
    "            if l[-3] == '_':\n",
    "                if l[:-3] in edge_net_lst:\n",
    "                    if l[:-3] not in edge_cur:\n",
    "                        edge_cur.append(l[:-3])\n",
    "    return(edge_cur)\n",
    "\n",
    "def get_taz(edge, edge_taz_dict):\n",
    "    try:\n",
    "        return(edge_taz_dict[edge])\n",
    "    except:\n",
    "        return(0)\n",
    "    \n",
    "def assign_edge_val(edge_cur, junctionid, edge_taz_dict):\n",
    "    edge_val = []\n",
    "    for edge in edge_cur:\n",
    "        val = get_taz(edge, edge_taz_dict)\n",
    "        if val == 0:\n",
    "            pass\n",
    "        else:\n",
    "            edge_val.append(val)\n",
    "    edge_val.append(junctionid)\n",
    "    return(edge_val)   \n",
    "  \n",
    "    \n",
    "#get edge id in tazs\n",
    "edge_taz_dict = {}\n",
    "edges_in2tazs = {}\n",
    "for tazs in taz_root.iter('taz'):\n",
    "    taz_id_cur = tazs.attrib['id']\n",
    "    for edge in tazs.findall('tazSource'):\n",
    "        edge_id_cur = edge.attrib['id']\n",
    "        if edge_id_cur not in edge_taz_dict:\n",
    "            edge_taz_dict[edge_id_cur] = taz_id_cur\n",
    "        else:\n",
    "            #print('warning, edges appear in more than one taz')\n",
    "            edges_in2tazs[edge_id_cur] = []\n",
    "            edges_in2tazs[edge_id_cur].append(edge_taz_dict[edge_id_cur])\n",
    "            edges_in2tazs[edge_id_cur].append(taz_id_cur)\n",
    "            \n",
    "print('edges in taz', edges_in2tazs)\n",
    "\n",
    "#get fcd data\n",
    "#check edges in the data\n",
    "edge_lst = []\n",
    "lane_lst = []\n",
    "\n",
    "for t in root.findall('timestep'):\n",
    "    #get lane id from vehicle\n",
    "    for veh in t.findall('vehicle'):\n",
    "        #print(veh.attrib['id'])\n",
    "        lane_id_cur = veh.attrib['lane']\n",
    "        #if '_' in lane_id_cur:\n",
    "        #    lane_id_cur = ExtractEdgeFromLane(lane_id_cur)\n",
    "        if lane_id_cur not in lane_lst:\n",
    "            lane_lst.append(lane_id_cur)\n",
    "    for per in t.findall('person'):\n",
    "        edge_id_cur = per.attrib['edge']\n",
    "        if edge_id_cur not in edge_lst:\n",
    "            edge_lst.append(edge_id_cur)\n",
    "            \n",
    "#convert lane to edge from net file\n",
    "lane_dict = {}\n",
    "edge_net_lst = [] #get all edges from the network\n",
    "for edge in net_root.findall('edge'):\n",
    "    edge_id = edge.attrib['id']\n",
    "    if edge_id not in edge_net_lst:\n",
    "        edge_net_lst.append(edge_id)   \n",
    "    for lane in edge.findall('lane'):\n",
    "        lane_id = lane.attrib['id']\n",
    "       # print(lane_id)\n",
    "        if lane_id in lane_lst and lane_id not in lane_dict:\n",
    "            lane_dict[lane_id] = edge_id\n",
    "#print(lane_dict)\n",
    "\n",
    "for edge in list(lane_dict.values()):\n",
    "    if edge not in edge_lst:\n",
    "        edge_lst.append(edge)\n",
    "        \n",
    "\n",
    "edges_missing_tazs = []\n",
    "edges_with_tazs = list(edge_taz_dict.keys())\n",
    "for edges in edge_lst:\n",
    "    if edges not in edges_with_tazs and edges not in edges_missing_tazs:\n",
    "        edges_missing_tazs.append(edges)\n",
    "        \n",
    "missings_taz_veh = []\n",
    "missings_taz_ped_pub = []\n",
    "for edges in edges_missing_tazs:\n",
    "    if \":\" in edges: \n",
    "        missings_taz_ped_pub.append(edges)\n",
    "    else:\n",
    "        missings_taz_veh.append(edges)\n",
    "        \n",
    "print('edge with tazs identified:', len(edges_with_tazs))\n",
    "print('edge w.o tazs identified:', len(edges_missing_tazs))\n",
    "\n",
    "#assign taz to missing edges\n",
    "missing_edge_connection = {}\n",
    "            \n",
    "for c in net_root.findall('connection'):\n",
    "    edge1 = c.attrib['from']\n",
    "    edge2 = c.attrib['to']\n",
    "    #print(edge1, edge2)\n",
    "    \n",
    "    if edge1 in edges_missing_tazs:\n",
    "        #print(edge1)\n",
    "        missing_edge_connection = assign_val(missing_edge_connection,\n",
    "                                             edge1, edge2)\n",
    "    elif edge2 in edges_missing_tazs:\n",
    "        #print(edge2)\n",
    "        missing_edge_connection = assign_val(missing_edge_connection,\n",
    "                                            edge2, edge1)\n",
    "        \n",
    "#print(len(list(missing_edge_connection.keys())))\n",
    "#print(len(edges_missing_tazs))\n",
    "#previous return = 6329 (both)\n",
    "\n",
    "print(len(list(missing_edge_connection.keys())))\n",
    "print(len(edges_missing_tazs))\n",
    "#previous return = 3203 (for 30 sec data)\n",
    "\n",
    "#for edges in missing_edge_connection:\n",
    "#print('test', convert2taz(missing_edge_connection['165086027#0']))\n",
    "\n",
    "missing_edge_taz = {}\n",
    "for edge in missing_edge_connection:\n",
    "    missing_edge_taz[edge] = convert2taz(missing_edge_connection[edge])\n",
    "    \n",
    "#get edge with no taz mapped\n",
    "edges_ = []\n",
    "index_ = 0\n",
    "for i in list(missing_edge_taz.values()):\n",
    "    if i == -1:\n",
    "        edges_.append(list(missing_edge_taz.keys())[index_])\n",
    "    index_ += 1\n",
    "    \n",
    "for edge in missing_edge_taz:\n",
    "    taz_val = missing_edge_taz[edge]\n",
    "    if taz_val != -1:\n",
    "        if edge not in edge_taz_dict:\n",
    "            edge_taz_dict[edge] = taz_val\n",
    "        else:\n",
    "            print('edge in taz already', edge)   \n",
    "            \n",
    "#check '_' in lanes and edges\n",
    "for i in edge_net_lst:\n",
    "    if i[-2] == '_':\n",
    "        pass\n",
    "    if i[-3] == '_':\n",
    "        pass\n",
    "    if i[-4] == '_':\n",
    "        print(i)\n",
    "\n",
    "\n",
    "#get junction\n",
    "missing_edge_junc = {}\n",
    "#index_ = 0\n",
    "for junc in net_root.findall('junction'):\n",
    "    #print(index_)\n",
    "    int_lane = junc.attrib['intLanes']\n",
    "    inc_lane = junc.attrib['incLanes']\n",
    "    junctionid = junc.attrib['id']\n",
    "\n",
    "    int_inc_lane = []\n",
    "    int_inc_lane.extend(sep_str(int_lane))\n",
    "    int_inc_lane.extend(sep_str(inc_lane))\n",
    "    #print(int_inc_lane)\n",
    "    if len(int_inc_lane) != 0:\n",
    "        edge_cur = get_edge(int_inc_lane, edge_net_lst)\n",
    "    for edge in edge_cur:\n",
    "        if edge in edges_:\n",
    "            if edge not in missing_edge_junc:\n",
    "                missing_edge_junc[edge] = assign_edge_val(edge_cur, \n",
    "                                                          junctionid, \n",
    "                                                          edge_taz_dict)            \n",
    "\n",
    "print(len(missing_edge_junc))\n",
    "selected_junc = list(missing_edge_junc.values())\n",
    "\n",
    "index_ = 0\n",
    "for edge in missing_edge_junc:\n",
    "    tazs = missing_edge_junc[edge][:-1]\n",
    "    #print(tazs)\n",
    "    taz_ct = []\n",
    "    unique_taz = np.unique(tazs)\n",
    "    #print(unique_taz)\n",
    "    for taz in unique_taz:\n",
    "        taz_ct.append(tazs.count(taz))\n",
    "    #print(taz_ct)\n",
    "    try:\n",
    "        taz_ = unique_taz[taz_ct.index(max(taz_ct))]\n",
    "    except:\n",
    "        taz_ = -1\n",
    "    #print(taz_)\n",
    "    if edge not in edge_taz_dict and taz_!=-1:\n",
    "        edge_taz_dict[edge] = taz_\n",
    "    elif taz_ == -1:\n",
    "        print(edge)\n",
    "#through the check, we assign 629 to the missing edges\n",
    "#edge_taz_dict[':53217375_5'] = '629'\n",
    "\n",
    "#missing_edge_junc[':53147426_0']\n",
    "#edge_taz_dict[':cluster_gneJ449_gneJ451_1']\n",
    "#edge_taz_dict[lane_dict['460423550#0_1']]\n",
    "\n",
    "#get total taz in sumo network\n",
    "taz_lst = []\n",
    "for taz in taz_root.findall('taz'):\n",
    "    taz_lst.append(taz.attrib['id'])\n",
    "    \n",
    "#def get_dt(fcd_outputpath, data_path, lane_dict, edge_taz_dict):\n",
    "    #use the fcd output to check the data\n",
    "#using 30 sec as an example\n",
    "#fcd_outputpath = 'fcd_output_10per_all_30sec.xml'\n",
    "#fcd_outputpath = 'fcd_output_10per_ht_30sec.xml'\n",
    "#fcd_file\n",
    "\n",
    "\n",
    "write_line = ''\n",
    "\n",
    "#tree = ET.parse(fcd_outputpath)\n",
    "#root = tree.getroot()\n",
    "with open (fcd_file[:-3]+'csv', 'w') as f:\n",
    "    for t in root.findall('timestep'):\n",
    "        time_cur = t.attrib['time']\n",
    "        #print(time_cur)\n",
    "        #get lane id from vehicle\n",
    "        for veh in t.findall('vehicle'):\n",
    "            vehid = veh.attrib['id']\n",
    "            #print(vehid)\n",
    "            lane_id_cur = veh.attrib['lane']\n",
    "            taz_id = edge_taz_dict[lane_dict[lane_id_cur]]\n",
    "            write_line = vehid+','+taz_id+','+time_cur+'\\n'\n",
    "            f.write(write_line)\n",
    "        try:    \n",
    "            for per in t.findall('person'):\n",
    "                perid = per.attrib['id']\n",
    "                #print(perid)\n",
    "                edge_id_cur = per.attrib['edge']\n",
    "                taz_id = edge_taz_dict[edge_id_cur]\n",
    "                write_line = perid+','+taz_id+','+time_cur+'\\n'\n",
    "                f.write(write_line)\n",
    "        except: \n",
    "            pass\n",
    "f.close()\n",
    "print('data output done')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data output done\n"
     ]
    }
   ],
   "source": [
    "#def get_dt(fcd_outputpath, data_path, lane_dict, edge_taz_dict):\n",
    "    #use the fcd output to check the data\n",
    "\n",
    "fcd_outputpath = 'fcd_output_5.xml'\n",
    "write_line = ''\n",
    "with open ('fcd_data_5.csv', 'w') as f:\n",
    "    for t in root.findall('timestep'):\n",
    "        time_cur = t.attrib['time']\n",
    "        #print(time_cur)\n",
    "        #get lane id from vehicle\n",
    "        for veh in t.findall('vehicle'):\n",
    "            vehid = veh.attrib['id']\n",
    "            #print(vehid)\n",
    "            lane_id_cur = veh.attrib['lane']\n",
    "            taz_id = edge_taz_dict[lane_dict[lane_id_cur]]\n",
    "            write_line = vehid+','+taz_id+','+time_cur+'\\n'\n",
    "            f.write(write_line)\n",
    "        try:    \n",
    "            for per in t.findall('person'):\n",
    "                \n",
    "                perid = per.attrib['id']\n",
    "                #print(perid)\n",
    "                edge_id_cur = per.attrib['edge']\n",
    "                taz_id = edge_taz_dict[edge_id_cur]\n",
    "                write_line = perid+','+taz_id+','+time_cur+'\\n'\n",
    "                f.write(write_line)\n",
    "        except: \n",
    "            pass\n",
    "f.close()\n",
    "print('data output done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shpfilepath = 'G:/My Drive/2020/Bias/data-processing/spatial_check/'\n",
    "zoneshp = gpd.read_file(shpfilepath+'Seattle_taz2010.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fcd_out_30' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfcd_out_30\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fcd_out_30' is not defined"
     ]
    }
   ],
   "source": [
    "fcd_out_30.sort_values(by=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_out_30.shape[0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fcd_obs(fcd_outdt):\n",
    "    test = fcd_outdt.groupby(['id']).size().to_frame()\n",
    "    test.reset_index(inplace=True)\n",
    "    \n",
    "    test.columns =['id', 'num_trip']\n",
    "    #print(test.sort_values(by=['num_trip'], ascending=False))\n",
    "    print(test.num_trip.describe())\n",
    "    test = test.sort_values(by=['num_trip'])\n",
    "    return(test)\n",
    "    \n",
    "def revise_fcd_dt(fcd_outpath):\n",
    "    fcd_outdt = pd.read_csv(fcd_outpath, header=None)\n",
    "    fcd_outdt.columns = ['id', 'TAZ', 'time']\n",
    "    fcd_outdt['time'] = fcd_outdt['time'].astype('int')\n",
    "    fcd_outdt['hr'] = (fcd_outdt['time'].values/3600).astype(int)\n",
    "    fcd_outdt['minus'] = (fcd_outdt['time'].values- fcd_outdt['hr'].values*3600)/60\n",
    "    fcd_outdt['time_30'] = (fcd_outdt['time']/30).astype(int)\n",
    "    fcd_outdt['time_5'] = (fcd_outdt['time']/300).astype(int)\n",
    "    return(fcd_outdt)\n",
    "    \n",
    "def get_test_trace(fcd_outdt, id_info, zoneshp, plot_trajec=True):\n",
    "    test_ = fcd_outdt[fcd_outdt['id'] == id_info]\n",
    "    test_obs = test_.groupby(by=['TAZ']).sum()\n",
    "    test_obs.reset_index(inplace=True)\n",
    "    \n",
    "    test_ = test_.sort_values(by=['id', 'time'])\n",
    "    hr = test_['hr'].values\n",
    "    minus = test_['minus'].values\n",
    "    \n",
    "    #check time span\n",
    "    print('start time:', [hr[0], minus[0]])\n",
    "    print('start time:', [hr[-1], minus[-1]])\n",
    "    \n",
    "    \n",
    "    merged = zoneshp.set_index('TAZ').join(test_obs.set_index('TAZ'))\n",
    "    merged.reset_index(inplace=True)\n",
    "    merged['traj_index'] = 0\n",
    "    merged['traj_index'][merged['TAZ'].isin(np.unique(test_.TAZ))] = 2\n",
    "    merged['traj_index'][merged['time']==min(test_obs['time'])] = 1\n",
    "    merged['traj_index'][merged['time']==max(test_obs['time'])] = 3\n",
    "    \n",
    "    if plot_trajec == True:\n",
    "        cmap_style = plt.cm.get_cmap('Blues', 4)\n",
    "        #selected_zone = np.unique(geoid_identify[:-4])\n",
    "        fig, ax1 = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "        # set the range for the choropleth\n",
    "        vmin, vmax = 0, 3\n",
    "        variable = 'traj_index'\n",
    "        merged.plot(column=variable, cmap=cmap_style, linewidth=0.8, \\\n",
    "                    ax=ax1, edgecolor='0.8')\n",
    "\n",
    "        #cbar.set_yticklabels(['None', 'Starting zone', 'Passed', 'Ending zone'])\n",
    "        ax3  = fig.add_axes([0.95,0.10,0.05,0.95])\n",
    "        norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "        cb1  = mpl.colorbar.ColorbarBase(ax3,cmap=cmap_style,norm=norm,orientation='vertical')\n",
    "        cb1.set_ticks([vmax/8,vmax*3./8,vmax*5./8,vmax*7./8])\n",
    "        cb1.set_ticklabels(['None', 'Starting zone', 'Passed', 'Ending zone'])\n",
    "        #saving our map as .png file.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_out = revise_fcd_dt('fcd_data_5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    70504.000000\n",
      "mean        20.253049\n",
      "std         16.835621\n",
      "min          1.000000\n",
      "25%          8.000000\n",
      "50%         15.000000\n",
      "75%         29.000000\n",
      "max        277.000000\n",
      "Name: num_trip, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fcd_obs_ = check_fcd_obs(fcd_out_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 in [3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             10_41979657\n",
       "1             13_49194874\n",
       "2           21_47580428_0\n",
       "3           24_47376848_0\n",
       "4            2_49192917_0\n",
       "                ...      \n",
       "1427916             39828\n",
       "1427917    480_5005_24409\n",
       "1427918    503_5009_24418\n",
       "1427919    480_5005_24409\n",
       "1427920    480_5005_24409\n",
       "Name: id, Length: 1427921, dtype: object"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcd_out_30['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to find any connection between ped and drive\n",
    "def check_id(split_id):\n",
    "    if len(split_id[0]) in [3, 4] and len(split_id[1]) in [3, 4]:\n",
    "        return(1)\n",
    "    \n",
    "id_veh = {}\n",
    "\n",
    "for i in fcd_obs_['id'].values:\n",
    "    if '_' in i:\n",
    "        id_ = i.split('_')\n",
    "        if len(id_)==3:\n",
    "            if check_id(id_) == 1:\n",
    "                id_veh[id_[2]] = i\n",
    "                \n",
    "id_ped = []\n",
    "for i in fcd_obs_['id'].values:\n",
    "    if '_' not in i:\n",
    "        id_ped.append(i)\n",
    "\n",
    "for i in id_ped:\n",
    "    if i in id_veh:\n",
    "        print(i, id_veh[i])\n",
    "        \n",
    "#after checking, it is found that the ped is doesn't match with the veh id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if we cover all the non-zero obs\n",
    "np.count_nonzero(obs_mx) == len(loc_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.134328358208955"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(obs_mx[np.where(obs_mx>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_5_obs['taz'] = time_5_obs['TAZ'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 5, 6, 7])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[5,6,7]])\n",
    "a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stay(dt, time_period, time_p_col):\n",
    "    dt[time_p_col] = (dt['time']/time_period).astype(int)\n",
    "    dt = dt.sort_values(by=['id', 'time'])\n",
    "    dt_id = dt['id'].values\n",
    "    dt_time = dt['time_p_col'].values\n",
    "    time_range = list(range(np.min(dt_time), np.max(dt_time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "86400/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_loc = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already in app_sim_method data\n",
    "fcd_dt_path = 'fcd_data_5.csv'\n",
    "#convert dt to matrix\n",
    "def convert2mx(csv_path, savepath, save_index=False):\n",
    "    fcd_out = pd.read_csv(fcd_dt_path, header=None)\n",
    "    fcd_out.columns = ['id', 'taz', 'time']\n",
    "    fcd_out.taz = fcd_out.taz.astype('int')\n",
    "    fcd_out.time = fcd_out.time.astype('int')\n",
    "    print('num id', len(np.unique(fcd_out.id)))\n",
    "\n",
    "    #fcd_out.sort_values(by=['time', 'taz'])\n",
    "    fcd_out['tl_id'] = fcd_out['taz'].astype(str)+'_'+((fcd_out['time']/300).astype(int)).astype(str)\n",
    "\n",
    "    fcd_out = fcd_out[~fcd_out['time'].isin([86400, 86700])]\n",
    "    time_ = np.unique((fcd_out['time']/300).astype(int))\n",
    "    newuser_ = {}\n",
    "    index_ = 0\n",
    "    for i in np.unique(fcd_out.id):\n",
    "        newuser_[i] = index_\n",
    "        index_ += 1\n",
    "\n",
    "    loc_time = {}\n",
    "    index_ = 0\n",
    "    taz_ = np.unique(fcd_out['taz'])\n",
    "    time_ = np.unique((fcd_out['time']/300).astype(int))\n",
    "    for taz in taz_:\n",
    "        for t in time_:\n",
    "            loc_time[str(taz)+'_'+str(t)] = index_\n",
    "            index_ += 1\n",
    "\n",
    "    origin_dt = np.zeros((len(newuser_), len(loc_time)))\n",
    "    ids_ = fcd_out['id'].values\n",
    "    tl_id_ = fcd_out['tl_id'].values\n",
    "    for i in range(len(ids_)):\n",
    "        origin_dt[newuser_[ids_[i]]][loc_time[tl_id_[i]]] = 1\n",
    "    if save_index == True:\n",
    "        pd.DataFrame(origin_dt).to_csv(savepath)\n",
    "    else:\n",
    "        return(origin_dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
