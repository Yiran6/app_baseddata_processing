{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ae476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "#import pandasql as ps\n",
    "import matplotlib.pyplot as plt\n",
    "#import shapefile as shp\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import pysal as ps\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import statistics\n",
    "\n",
    "#temporal pattern clustering\n",
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from sklearn import metrics\n",
    "\n",
    "#!pip install dtaidistance\n",
    "#!pip install plotly\n",
    "#!pip install tslearn\n",
    "#source https://dtaidistance.readthedocs.io/en/latest/usage/dtw.html\n",
    "#from dtaidistance import dtw\n",
    "#from dtaidistance import dtw_visualisation as dtwvis\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "#regression model\n",
    "#from patsy import dmatrices\n",
    "#import statsmodels.api as sm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#from pysal.model import spreg\n",
    "#from pysal.lib import weights\n",
    "#from pysal.explore import esda\n",
    "\n",
    "#calculate the demographic bias \n",
    "#use wasserstein_distance\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "#path = 'C:/Users/29700/Downloads/RecSys-Workshop/tutorials/'\n",
    "#os.chdir(path)\n",
    "#import utilities as utl\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mae\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import time\n",
    "import scipy.sparse\n",
    "from scipy.stats import entropy\n",
    "from sys import exit\n",
    "\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "import math\n",
    "\n",
    "import ast\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mae\n",
    "\n",
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1050705b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38343</td>\n",
       "      <td>47.596215</td>\n",
       "      <td>-121.979614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13824</td>\n",
       "      <td>47.836323</td>\n",
       "      <td>-122.296098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18910</td>\n",
       "      <td>47.222268</td>\n",
       "      <td>-122.493760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15894</td>\n",
       "      <td>47.613766</td>\n",
       "      <td>-122.342089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5157</td>\n",
       "      <td>47.313734</td>\n",
       "      <td>-122.424222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110875</th>\n",
       "      <td>453437</td>\n",
       "      <td>47.330805</td>\n",
       "      <td>-122.170224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110876</th>\n",
       "      <td>455318</td>\n",
       "      <td>47.322828</td>\n",
       "      <td>-122.282397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110877</th>\n",
       "      <td>450532</td>\n",
       "      <td>47.158069</td>\n",
       "      <td>-122.462895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110878</th>\n",
       "      <td>454334</td>\n",
       "      <td>47.944066</td>\n",
       "      <td>-122.224009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110879</th>\n",
       "      <td>454449</td>\n",
       "      <td>47.807490</td>\n",
       "      <td>-122.321711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110880 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1           2\n",
       "0        38343  47.596215 -121.979614\n",
       "1        13824  47.836323 -122.296098\n",
       "2        18910  47.222268 -122.493760\n",
       "3        15894  47.613766 -122.342089\n",
       "4         5157  47.313734 -122.424222\n",
       "...        ...        ...         ...\n",
       "110875  453437  47.330805 -122.170224\n",
       "110876  455318  47.322828 -122.282397\n",
       "110877  450532  47.158069 -122.462895\n",
       "110878  454334  47.944066 -122.224009\n",
       "110879  454449  47.807490 -122.321711\n",
       "\n",
       "[110880 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(home_path, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0700339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "#read latlon id with home identified\n",
    "def read_data(filepath, homepath, combine_dt):\n",
    "    #read ids with home identified\n",
    "    id_withhome = pd.read_csv(homepath, header=None, sep='\\t')\n",
    "    id_withhome.columns = ['newid','lat','lon']\n",
    "    if combine_dt == True:\n",
    "        id_latlon = pd.read_csv(filepath, header=0, sep=',')\n",
    "        #get ids\n",
    "        homeids = list(np.unique(id_withhome.newid))\n",
    "        id_latlon = id_latlon[id_latlon.newid.isin(homeids)]\n",
    "        return(id_latlon)\n",
    "    else:\n",
    "        return(id_withhome)\n",
    "    #print(id_latlon.head(3))\n",
    "    \n",
    "def convert_to_shp(filepath, homepath, combine_dt=True):\n",
    "    points = read_data(filepath, homepath, combine_dt)\n",
    "    geo_points = gpd.GeoDataFrame(points,\n",
    "                                 geometry=gpd.points_from_xy(points['lon'],\\\n",
    "                                                           points['lat']),\\\n",
    "                                crs={'init': 'epsg:4326'})\n",
    "    return(geo_points)\n",
    "\n",
    "#make sure the default crs for zone file is espg4326\n",
    "def spatial_join(zone_path, points):\n",
    "    Zone = gpd.read_file(zone_path)\n",
    "    Zone.crs = points.crs\n",
    "    points = gpd.tools.sjoin(points, Zone, how='right')\n",
    "    return(points)\n",
    "\n",
    "#conduct a small test to make sure the spatial join works\n",
    "def small_sjoin_test(zonepath, data, key_col):\n",
    "    test_dt = spatial_join(zonepath, data.iloc[:15])\n",
    "    geoid = np.unique(test_dt[key_col])\n",
    "    if 'nan' in geoid:\n",
    "        print('join not successfully')\n",
    "        print(geoid)\n",
    "    else:\n",
    "        print(geoid)\n",
    "    return(test_dt)\n",
    "    \n",
    "#plot zone and points to make sure they are in the same projection\n",
    "def plot_point_zone(zone_path, points):\n",
    "    zone = gpd.read_file(zone_path)\n",
    "    base = zone.plot(color='white', edgecolor='grey')\n",
    "    points.plot(ax=base, marker='o', color='red', markersize=5)\n",
    "    \n",
    "#check null data after spatial join\n",
    "def checknull(data_shape, zone_path, savepath, save_index=True):\n",
    "    data_shape['geoid'] = data_shape['GEOID10'].tolist()\n",
    "    null_Data = data_shape[data_shape['geoid'].isnull()]\n",
    "    print(len(null_Data['geoid']))\n",
    "    plot_point_zone(zone_path, null_Data)\n",
    "    if save_index == True:\n",
    "        null_Data.to_csv(savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f9732",
   "metadata": {},
   "source": [
    "## Spatial join process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d88b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREVIOUS CODE\n",
    "paths = 'G:/My Drive/2021/Bias/PSRC_od_simulation_2018/'\n",
    "input_path = paths+'taz2010_revised.shp'\n",
    "taz = gpd.read_file(input_path)\n",
    "\n",
    "home_path = 'G:/My Drive/2020/Bias/data-processing/'+'cuebiq_home_psrc_raw.csv'\n",
    "home_shp = convert_to_shp('G:/', home_path, combine_dt=False)\n",
    "home_withtaz = spatial_join(input_path, home_shp)\n",
    "\n",
    "home_withtaz.to_csv(paths+'home_psrc_withtaz.csv')\n",
    "\n",
    "#get Seattle home ids\n",
    "os.chdir('G:/My Drive/2020/Bias/data-processing/')\n",
    "home_withct = spatial_join('spatial_check/Seattle_ct2010.shp', home_shp)\n",
    "Seattle_homeids = sorted(home_withct['newid'].values)\n",
    "\n",
    "#latlon_shape_ct = latlon_shape_ct[~latlon_shape_ct['GEOID10'].isnull()]\n",
    "#latlon_shape_ct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52be335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on sumo simulation scale\n",
    "selected_taz = [426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
    "                439, 440, 441, 442, 443, 446, 447, 448, 450, 451, 452, 453, 454,\n",
    "                455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, \n",
    "                468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 479, 480, 481, \n",
    "                482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, \n",
    "                495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, \n",
    "                508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, \n",
    "                521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n",
    "                534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546,\n",
    "                547, 548, 549, 550, 551, 552, 553, 566, 567, 568, 569, 570, 571,\n",
    "                572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
    "                604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 627,\n",
    "                628, 629, 630, 631, 632, 634, 635, 636, 650, 651, 652, 653]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7ac0f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_left</th>\n",
       "      <th>newid</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>TAZ</th>\n",
       "      <th>COUNTY_FIP</th>\n",
       "      <th>COUNTY_NM</th>\n",
       "      <th>Area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3480.0</td>\n",
       "      <td>11698.0</td>\n",
       "      <td>47.720013</td>\n",
       "      <td>-122.372924</td>\n",
       "      <td>1</td>\n",
       "      <td>033</td>\n",
       "      <td>King</td>\n",
       "      <td>7117998.77</td>\n",
       "      <td>POLYGON ((-122.36094 47.72734, -122.36095 47.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3643.0</td>\n",
       "      <td>24151.0</td>\n",
       "      <td>47.723837</td>\n",
       "      <td>-122.362050</td>\n",
       "      <td>1</td>\n",
       "      <td>033</td>\n",
       "      <td>King</td>\n",
       "      <td>7117998.77</td>\n",
       "      <td>POLYGON ((-122.36094 47.72734, -122.36095 47.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14927.0</td>\n",
       "      <td>54927.0</td>\n",
       "      <td>47.728959</td>\n",
       "      <td>-122.362530</td>\n",
       "      <td>1</td>\n",
       "      <td>033</td>\n",
       "      <td>King</td>\n",
       "      <td>7117998.77</td>\n",
       "      <td>POLYGON ((-122.36094 47.72734, -122.36095 47.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_left    newid        lat         lon  TAZ COUNTY_FIP COUNTY_NM  \\\n",
       "0      3480.0  11698.0  47.720013 -122.372924    1        033      King   \n",
       "0      3643.0  24151.0  47.723837 -122.362050    1        033      King   \n",
       "0     14927.0  54927.0  47.728959 -122.362530    1        033      King   \n",
       "\n",
       "         Area                                           geometry  \n",
       "0  7117998.77  POLYGON ((-122.36094 47.72734, -122.36095 47.7...  \n",
       "0  7117998.77  POLYGON ((-122.36094 47.72734, -122.36095 47.7...  \n",
       "0  7117998.77  POLYGON ((-122.36094 47.72734, -122.36095 47.7...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_withtaz.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cc3affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data('withobs04052017.csv','cuebiq_home_psrc_raw.csv','spatial_check/ct2010.shp')\n",
    "\n",
    "def get_new_hr_minut(hr, minut):\n",
    "    min_index = math.ceil(minut/5)\n",
    "    if min_index != 12:\n",
    "        return(hr, min_index*5)\n",
    "    else: \n",
    "        if hr+1 != 24:\n",
    "            return(hr+1, 0)\n",
    "        else: \n",
    "            return(0, 0)\n",
    "    \n",
    "def get_total_locts(obs_data):\n",
    "    locts = {}\n",
    "    num_index = 0\n",
    "    for geoid in np.unique(obs_data['GEOID10']):\n",
    "        for hr in range(24):\n",
    "            for minut in np.unique(obs_data['min_5']):\n",
    "                a = str(geoid)+'_'+str(hr)+'_'+str(minut)\n",
    "                locts[a] = num_index\n",
    "                num_index += 1\n",
    "    nb_locts = len(locts)\n",
    "    return(locts, nb_locts)\n",
    "\n",
    "def get_data(obs_data_file, home_file, zone_file, newids, newid_dict):\n",
    "    latlon_shape = convert_to_shp(obs_data_file, home_file)\n",
    "    print('original data shape', latlon_shape.shape)\n",
    "    latlon_shape = spatial_join(zone_file, latlon_shape)\n",
    "    print('data shape after data join', latlon_shape.shape)\n",
    "    latlon_shape = latlon_shape[~latlon_shape['GEOID10'].isnull()]\n",
    "    print('data shape after deleting nulls',latlon_shape.shape)\n",
    "    \n",
    "    #summary the obs\n",
    "    #revise the data, revised as 5-min \n",
    "    new_hr = []\n",
    "    minut_5 = []\n",
    "    for hr, minut in latlon_shape[['hr', 'minut']].values:\n",
    "        h, min_5 = get_new_hr_minut(hr, minut)\n",
    "        new_hr.append(h)\n",
    "        minut_5.append(min_5)\n",
    "    \n",
    "    latlon_shape['newh'] = new_hr\n",
    "    latlon_shape['min_5'] = minut_5\n",
    "\n",
    "    group_col = ['newid','GEOID10', 'd', 'newh', 'min_5']\n",
    "    latlon_shape = latlon_shape[['newid','GEOID10', 'd', 'newh', 'min_5', 'obs']].groupby(by=group_col).sum()\n",
    "    latlon_shape.reset_index(inplace=True)\n",
    "    latlon_shape.sort_values(by=group_col)\n",
    "    \n",
    "    cur_newids = list(np.unique(latlon_shape['newid']))\n",
    "    add_ids = []\n",
    "    for ids in cur_newids:\n",
    "        if ids not in newids:\n",
    "            newids.append(ids)\n",
    "            add_ids.append(ids)\n",
    "    n_newid = len(newids)\n",
    "    \n",
    "    if add_ids != []:\n",
    "        cur_index = len(newid_dict)\n",
    "        for ids in add_ids:\n",
    "            newid_dict[ids] = cur_index \n",
    "            cur_index += 1\n",
    "    \n",
    "    stids, n_stid = get_total_locts(latlon_shape)\n",
    "    latlon_shape['stids'] = latlon_shape['GEOID10']+'_'+latlon_shape['newh'].astype(str)+'_'+latlon_shape['min_5'].astype(str)\n",
    "    \n",
    "    obs_matrix = np.zeros((n_newid, n_stid))\n",
    "    for ids, stid, obs in latlon_shape[['newid', 'stids', 'obs']].values:\n",
    "        obs_matrix[newid_dict[ids], stids[stid]] = obs\n",
    "    \n",
    "    print('generate obs matrix shape', obs_matrix.shape)\n",
    "    path = 'G:/My Drive/2021/Bias/SUMO_simulation/'\n",
    "    savefile = 'obsmx'+obs_data_file[7:-4]+'.csv'\n",
    "    pd.DataFrame(obs_matrix).to_csv(path+savefile)\n",
    "    \n",
    "    return(newids, newid_dict)\n",
    "\n",
    "def get_total_locts_taz(obs_data):\n",
    "    locts = {}\n",
    "    num_index = 0\n",
    "    for geoid in np.unique(obs_data['TAZ']):\n",
    "        for hr in range(24):\n",
    "            for minut in np.unique(obs_data['min_5']):\n",
    "                a = str(geoid)+'_'+str(hr)+'_'+str(minut)\n",
    "                locts[a] = num_index\n",
    "                num_index += 1\n",
    "    nb_locts = len(locts)\n",
    "    return(locts, nb_locts)\n",
    "\n",
    "def get_data_taz(obs_data_file, home_file, zone_file, newids, newid_dict):\n",
    "    latlon_shape = convert_to_shp(obs_data_file, home_file)\n",
    "    print('original data shape', latlon_shape.shape)\n",
    "    latlon_shape = spatial_join(zone_file, latlon_shape)\n",
    "    print('data shape after data join', latlon_shape.shape)\n",
    "    latlon_shape = latlon_shape[~latlon_shape['TAZ'].isnull()]\n",
    "    print('data shape after deleting nulls',latlon_shape.shape)\n",
    "    \n",
    "    #summary the obs\n",
    "    #revise the data, revised as 5-min \n",
    "    new_hr = []\n",
    "    minut_5 = []\n",
    "    for hr, minut in latlon_shape[['hr', 'minut']].values:\n",
    "        h, min_5 = get_new_hr_minut(hr, minut)\n",
    "        new_hr.append(h)\n",
    "        minut_5.append(min_5)\n",
    "    \n",
    "    latlon_shape['newh'] = new_hr\n",
    "    latlon_shape['min_5'] = minut_5\n",
    "\n",
    "    group_col = ['newid','TAZ', 'd', 'newh', 'min_5']\n",
    "    latlon_shape = latlon_shape[['newid','TAZ', 'd', 'newh', 'min_5', 'obs']].groupby(by=group_col).sum()\n",
    "    latlon_shape.reset_index(inplace=True)\n",
    "    latlon_shape.sort_values(by=group_col)\n",
    "    \n",
    "    cur_newids = list(np.unique(latlon_shape['newid']))\n",
    "    add_ids = []\n",
    "    for ids in cur_newids:\n",
    "        if ids not in newids:\n",
    "            newids.append(ids)\n",
    "            add_ids.append(ids)\n",
    "    n_newid = len(newids)\n",
    "    \n",
    "    if add_ids != []:\n",
    "        cur_index = len(newid_dict)\n",
    "        for ids in add_ids:\n",
    "            newid_dict[ids] = cur_index \n",
    "            cur_index += 1\n",
    "    \n",
    "    stids, n_stid = get_total_locts_taz(latlon_shape)\n",
    "    latlon_shape['stids'] = latlon_shape['TAZ']+'_'+latlon_shape['newh'].astype(str)+'_'+latlon_shape['min_5'].astype(str)\n",
    "    \n",
    "    obs_matrix = np.zeros((n_newid, n_stid))\n",
    "    for ids, stid, obs in latlon_shape[['newid', 'stids', 'obs']].values:\n",
    "        obs_matrix[newid_dict[ids], stids[stid]] = obs\n",
    "    \n",
    "    print('generate obs matrix shape', obs_matrix.shape)\n",
    "    path = 'G:/My Drive/2021/Bias/SUMO_simulation/'\n",
    "    savefile = 'obstazmx'+obs_data_file[7:-4]+'.csv'\n",
    "    pd.DataFrame(obs_matrix).to_csv(path+savefile)\n",
    "    \n",
    "    return(newids, newid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c12a2d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:/My Drive/2021/Bias/PSRC_od_simulation_2018/taz2010_revised.shp'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22b64a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yiran\\anaconda3\\lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "#get_data_taz('withobs04052017.csv','cuebiq_home_psrc_raw.csv',input_path)\n",
    "latlon_shape = convert_to_shp('withobs04052017.csv', 'cuebiq_home_psrc_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_data_taz(obs_data_file, home_file, zone_file, newids, newid_dict):\n",
    "\n",
    "print('original data shape', latlon_shape.shape)\n",
    "latlon_shape = spatial_join(zone_file, latlon_shape)\n",
    "print('data shape after data join', latlon_shape.shape)\n",
    "latlon_shape = latlon_shape[~latlon_shape['TAZ'].isnull()]\n",
    "print('data shape after deleting nulls',latlon_shape.shape)\n",
    "\n",
    "#summary the obs\n",
    "#revise the data, revised as 5-min \n",
    "new_hr = []\n",
    "minut_5 = []\n",
    "for hr, minut in latlon_shape[['hr', 'minut']].values:\n",
    "    h, min_5 = get_new_hr_minut(hr, minut)\n",
    "    new_hr.append(h)\n",
    "    minut_5.append(min_5)\n",
    "\n",
    "latlon_shape['newh'] = new_hr\n",
    "latlon_shape['min_5'] = minut_5\n",
    "\n",
    "group_col = ['newid','TAZ', 'd', 'newh', 'min_5']\n",
    "latlon_shape = latlon_shape[['newid','TAZ', 'd', 'newh', 'min_5', 'obs']].groupby(by=group_col).sum()\n",
    "latlon_shape.reset_index(inplace=True)\n",
    "latlon_shape.sort_values(by=group_col)\n",
    "\n",
    "cur_newids = list(np.unique(latlon_shape['newid']))\n",
    "add_ids = []\n",
    "for ids in cur_newids:\n",
    "    if ids not in newids:\n",
    "        newids.append(ids)\n",
    "        add_ids.append(ids)\n",
    "n_newid = len(newids)\n",
    "\n",
    "if add_ids != []:\n",
    "    cur_index = len(newid_dict)\n",
    "    for ids in add_ids:\n",
    "        newid_dict[ids] = cur_index \n",
    "        cur_index += 1\n",
    "\n",
    "stids, n_stid = get_total_locts_taz(latlon_shape)\n",
    "latlon_shape['stids'] = latlon_shape['TAZ']+'_'+latlon_shape['newh'].astype(str)+'_'+latlon_shape['min_5'].astype(str)\n",
    "\n",
    "obs_matrix = np.zeros((n_newid, n_stid))\n",
    "for ids, stid, obs in latlon_shape[['newid', 'stids', 'obs']].values:\n",
    "    obs_matrix[newid_dict[ids], stids[stid]] = obs\n",
    "\n",
    "print('generate obs matrix shape', obs_matrix.shape)\n",
    "path = 'G:/My Drive/2021/Bias/SUMO_simulation/'\n",
    "savefile = 'obstazmx'+obs_data_file[7:-4]+'.csv'\n",
    "pd.DataFrame(obs_matrix).to_csv(path+savefile)\n",
    "\n",
    "return(newids, newid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cuebiq data file name ('withobs04052017.csv')\n",
    "obs_csv_file = []\n",
    "for i in os.listdir():\n",
    "    if 'withobs' in i:\n",
    "        obs_csv_file.append(i)\n",
    "\n",
    "#GET THE DATA\n",
    "newids = []\n",
    "newid_dict = {}\n",
    "for obs_file in obs_csv_file:\n",
    "    newids, newid_dict = get_data(obs_file, 'cuebiq_home_psrc_raw.csv', 'spatial_check/Seattle_ct2010.shp', newids, newid_dict) \n",
    "    \n",
    "with open('newid_tsid_aggregation.txt', 'w') as f:\n",
    "    f.write(str(newid_dict))\n",
    "    f.write(str(stids))\n",
    "f.close()\n",
    "\n",
    "#n_user = len(newids)\n",
    "def get_agg_matrix(input_path, n_user, obs_matrix, first_input=True):\n",
    "    cur_obs_matrix = pd.read_csv(input_path, index_col=0).to_numpy()\n",
    "    \n",
    "    cur_n_user, n_sid = cur_obs_matrix.shape\n",
    "    #print('current obs matrix shape', cur_obs_matrix.shape)\n",
    "        \n",
    "    if cur_n_user != n_user:\n",
    "        diff_n_user = int(n_user-cur_n_user)\n",
    "        filled_zero_matrix = np.zeros((diff_n_user, n_sid))\n",
    "        cur_obs_matrix = np.vstack([cur_obs_matrix, filled_zero_matrix])\n",
    "        #print('revised matrix shape', cur_obs_matrix.shape)\n",
    "    if first_input == False:\n",
    "        obs_matrix_revised = obs_matrix + cur_obs_matrix\n",
    "    else:\n",
    "        obs_matrix_revised = cur_obs_matrix\n",
    "    return(obs_matrix_revised)\n",
    "\n",
    "#get_sum_matrix \n",
    "input_path = 'G:/My Drive/2021/Bias/SUMO_simulation/' \n",
    "matrixfile = []\n",
    "for i in os.listdir(input_path):\n",
    "    if 'obsmx' in i:\n",
    "        matrixfile.append(i)\n",
    "        \n",
    "obs_mx = 0\n",
    "for mxfile in matrixfile:\n",
    "    filepath = input_path + mxfile\n",
    "    if obs_mx == 0:\n",
    "        obs_matrix_sum = get_agg_matrix(filepath, n_user, obs_mx, True)\n",
    "        obs_mx = 1\n",
    "        #print('matrixfile cur obs matrix shape', obs_matrix_sum.shape)\n",
    "    else:\n",
    "        obs_matrix_sum = get_agg_matrix(filepath, n_user, obs_matrix_sum, False)\n",
    "        #obs_mx = obs_matrix.copy()\n",
    "        #print('matrixfile cur obs matrix shape', obs_matrix_sum.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
