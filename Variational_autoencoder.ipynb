{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import ast\n",
    "import time\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#for AE \n",
    "#kernel = Python 3\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.parallel\n",
    "#import torch.optim as optim\n",
    "#import torch.utils.data\n",
    "\n",
    "#from torch.nn import functional\n",
    "#from torch.autograd import Variable\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import timeit\n",
    "#Source\n",
    "#VAE keras: https://keras.io/examples/generative/vae/\n",
    "#VAE CF: https://github.com/kilolgupta/Variational-Autoencoders-Collaborative-Filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\n",
    "* [VAE keras](https://keras.io/examples/generative/vae/) (for 3d image data inputs)\n",
    "* [VAE CF](https://github.com/kilolgupta/Variational-Autoencoders-Collaborative-Filtering)\n",
    "* [VAE for CF](https://github.com/dawenl/vae_cf/blob/master/VAE_ML20M_WWW2018.ipynb)\n",
    "\n",
    "[Activation function discussion](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlanduse_input_path = 'G:/My Drive/2021/Bias/census_block_data/'\\nland_use_obs = pd.read_csv('obs_landuse.csv', header=None).to_numpy()\\nland_use_01 =  pd.read_csv('obs01_landuse.csv', header=None).to_numpy()\\n\\nland_use_zero_index = pd.read_csv('zero_index_landuse.csv',header=None).to_numpy()\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data processing\n",
    "#get data based on census tract\n",
    "#based on downtown Seattle\n",
    "#test data\n",
    "os.chdir('G:/My Drive/2021/Bias/')\n",
    "\n",
    "test_matrice_reverse = pd.read_csv('test_matrice_reverse.csv', header=None).to_numpy()\n",
    "Zero_index = pd.read_csv('Zero_index.csv', header=None).to_numpy()\n",
    "#create a 0-1 value matrice\n",
    "test_matrice_reverse_01 = np.where(test_matrice_reverse[:, 1:]>0, 1, 0)\n",
    "#os.listsdir()\n",
    "\n",
    "#land use data \n",
    "#obs\n",
    "#obs_01\n",
    "#zero index\n",
    "'''\n",
    "landuse_input_path = 'G:/My Drive/2021/Bias/census_block_data/'\n",
    "land_use_obs = pd.read_csv('obs_landuse.csv', header=None).to_numpy()\n",
    "land_use_01 =  pd.read_csv('obs01_landuse.csv', header=None).to_numpy()\n",
    "\n",
    "land_use_zero_index = pd.read_csv('zero_index_landuse.csv',header=None).to_numpy()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004185859929712125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparsity of data in the paper\n",
    "20000263/(27278*138493)\n",
    "\n",
    "#sparsity in the cuebiq data\n",
    "np.count_nonzero(test_matrice_reverse[:, 1:])/(7767*3456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE CF: https://github.com/kilolgupta/Variational-Autoencoders-Collaborative-Filtering\n",
    "test_path = 'G:/My Drive/2021/Bias/recsys_test/'\n",
    "os.chdir(test_path)\n",
    "\n",
    "ratings = pd.read_csv('ml-20m/ratings.csv')\n",
    "movies = pd.read_csv('ml-20m/movies.csv')\n",
    "links = pd.read_csv('ml-20m/links.csv')\n",
    "\n",
    "movie_id = np.unique(movies['movieId'])\n",
    "ratings = ratings[ratings['movieId'].isin(movie_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_users = len(unique_user_ids)\n",
    "print(\"total number of users: \", number_of_users)\n",
    "\n",
    "number_of_movies = len(unique_movie_ids)\n",
    "print(\"total number of movies: \", number_of_movies)\n",
    "\n",
    "# split users into training, validation and test\n",
    "val_user_ids = []\n",
    "test_user_ids = []\n",
    "train_user_ids = []\n",
    "\n",
    "for i in range(10000):\n",
    "    val_user_ids.append(unique_user_ids[i]) # first 10k after shuffling keys\n",
    "\n",
    "for i in range(10000, 20000):\n",
    "    test_user_ids.append(unique_user_ids[i]) # next 10k after shuffling keys\n",
    "\n",
    "for i in range(20000, number_of_users): # all the remaining form training data\n",
    "    train_user_ids.append(unique_user_ids[i])\n",
    "\n",
    "\n",
    "# creating a movieId and userId to index dictionary for creating train_data ndarray\n",
    "movie2id = {}\n",
    "movie2id = dict((mid, i) for (i, mid) in enumerate(unique_movie_ids))\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"creating training data....\")\n",
    "\n",
    "user2id = {}\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(train_user_ids))\n",
    "\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "for u_id in train_user_ids:\n",
    "    print(\"train-\",user2id[u_id])\n",
    "    m_ids = raw_data[(raw_data.userId == u_id) & (raw_data.rating > 3.5)]['movieId'].tolist()\n",
    "    movie_indexes = [movie2id[m] for m in m_ids]\n",
    "    rows.extend([user2id[u_id] for i in range(len(m_ids))])\n",
    "    cols.extend(movie_indexes)\n",
    "\n",
    "# pickle.dump(rows, open(\"rows.file\", \"wb\"))\n",
    "# pickle.dump(cols, open(\"cols.file\", \"wb\"))\n",
    "\n",
    "\n",
    "# creating a sparse matrix with no_of_train_users X movies for training, binarized feedback\n",
    "# rows and cols should be of same length\n",
    "train_data = sparse.csr_matrix((np.ones_like(rows),(np.array(rows), np.array(cols))), dtype='float64', shape=(len(train_user_ids), number_of_movies))\n",
    "\n",
    "# dumping variable to load later for use in VAE code\n",
    "pickle.dump(train_data, open(\"train_data.file\", \"wb\"))\n",
    "print(\"number of training users: \", len(train_user_ids))\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"creating test data....\")\n",
    "test_user2id = {}\n",
    "test_user2id = dict((uid, i) for (i, uid) in enumerate(test_user_ids))\n",
    "\n",
    "test_rows = []\n",
    "test_cols = []\n",
    "for u_id in test_user_ids:\n",
    "    print(\"test-\", test_user2id[u_id])\n",
    "    m_ids = raw_data[(raw_data.userId == u_id) & (raw_data.rating > 3.5)]['movieId'].tolist()\n",
    "    movie_indexes = [movie2id[m] for m in m_ids]\n",
    "    test_rows.extend([test_user2id[u_id] for i in range(len(m_ids))])\n",
    "    test_cols.extend(movie_indexes)\n",
    "\n",
    "test_data = sparse.csr_matrix((np.ones_like(test_rows),(np.array(test_rows), np.array(test_cols))), dtype='float64', shape=(len(test_user_ids), number_of_movies))\n",
    "pickle.dump(test_data, open(\"test_data.file\", \"wb\"))\n",
    "print(\"number of test users: \", len(test_user_ids))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(\"creating validation data\")\n",
    "val_user2id = {}\n",
    "val_user2id = dict((uid, i) for (i, uid) in enumerate(val_user_ids))\n",
    "\n",
    "val_rows = []\n",
    "val_cols = []\n",
    "for u_id in val_user_ids:\n",
    "    print(\"val-\", val_user2id[u_id])\n",
    "    m_ids = raw_data[(raw_data.userId == u_id) & (raw_data.rating > 3.5)]['movieId'].tolist()\n",
    "    movie_indexes = [movie2id[m] for m in m_ids]\n",
    "    val_rows.extend([val_user2id[u_id] for i in range(len(m_ids))])\n",
    "    val_cols.extend(movie_indexes)\n",
    "\n",
    "val_data = sparse.csr_matrix((np.ones_like(val_rows),(np.array(val_rows), np.array(val_cols))), dtype='float64', shape=(len(val_user_ids), number_of_movies))\n",
    "pickle.dump(val_data, open(\"val_data.file\", \"wb\"))\n",
    "print(\"number of validation users: \", len(val_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "history = LossHistory()\n",
    "\n",
    "# encoder/decoder network size\n",
    "batch_size=500\n",
    "original_dim=26621 # number of movies\n",
    "intermediate_dim=600\n",
    "latent_dim=200\n",
    "nb_epochs=20\n",
    "epsilon_std=1.0\n",
    "\n",
    "\n",
    "# encoder network\n",
    "x=Input(batch_shape=(batch_size,original_dim))\n",
    "h=Dense(intermediate_dim, activation='tanh')(x)\n",
    "z_mean=Dense(latent_dim)(h)\n",
    "z_log_var=Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "# sampling from latent dimension for decoder/generative part of network\n",
    "def sampling(args):\n",
    "    _mean,_log_var=args\n",
    "    epsilon=K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., std=epsilon_std)\n",
    "    return _mean+K.exp(_log_var/2)*epsilon\n",
    "\n",
    "z= Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# decoder network\n",
    "h_decoder=Dense(intermediate_dim, activation='tanh')\n",
    "x_bar=Dense(original_dim,activation='softmax') # this should be softmax right?\n",
    "h_decoded = h_decoder(z)\n",
    "x_decoded = x_bar(h_decoded)\n",
    "\n",
    "# build and compile model\n",
    "vae = Model(x, x_decoded)\n",
    "def vae_loss(x,x_bar):\n",
    "    reconst_loss=original_dim*objectives.binary_crossentropy(x, x_bar)\n",
    "    kl_loss= -0.5*K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return kl_loss + reconst_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "\n",
    "x_train = pickle.load( open( \"train_data.file\", \"rb\" ) )\n",
    "print(\"number of training users: \", x_train.shape[0])\n",
    "\n",
    "x_val = pickle.load( open( \"val_data.file\", \"rb\" ) )\n",
    "x_val = x_val.todense()\n",
    "\n",
    "def nn_batch_generator(x, y, batch_size, samples_per_epoch):\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    x =  x[shuffle_index, :]\n",
    "    y =  y[shuffle_index, :]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        x_batch = x[index_batch,:].todense()\n",
    "        y_batch = y[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield (np.array(x_batch),np.array(y_batch))\n",
    "        if (counter >= number_of_batches):\n",
    "            counter=0\n",
    "\n",
    "\n",
    "weightsPath = \"./tmp/weights.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=weightsPath, verbose=1, save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# sending complete training data and shuffle flag will shuffle so that each user comes atleast once in training because of multiple epochs\n",
    "vae.fit_generator(nn_batch_generator(x_train, x_train, batch_size, 118000), samples_per_epoch=118000, nb_epoch=nb_epochs, \n",
    "    validation_data=(x_val, x_val), callbacks=[checkpointer, reduce_lr, history])\n",
    "\n",
    "print(\"training losses over epochs\")\n",
    "print(history.losses)\n",
    "\n",
    "print(\"validation losses over epochs\")\n",
    "print(history.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
