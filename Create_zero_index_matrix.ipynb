{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc98049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import osmnx as ox\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#import networkx as nx\n",
    "from datetime import datetime\n",
    "from shapely.geometry import LineString, Point\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15105555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('G:/My Drive/2021/Bias/census_tract_process/')\n",
    "os.chdir('G:/My Drive/2021/Bias/census_block_data/')\n",
    "def check_missing_routes(est_rt, join_rt):\n",
    "    num_rt = len(np.unique(est_rt['osmids']))\n",
    "    num_join_rt = len(np.unique(join_rt['osmids']))\n",
    "    if num_rt != num_join_rt:\n",
    "        print(num_rt, num_join_rt, num_rt-num_join_rt)\n",
    "        #the missing_rt using set is different\n",
    "        #missing_rt = set(np.unique(est_rt['osmids'])) ^ set(np.unique(join_rt['osmids']))\n",
    "        return(missing_rt)\n",
    "    else:\n",
    "        return(None)\n",
    "    \n",
    "def convert_routetype(routes):\n",
    "    if routes[-1] == ',':\n",
    "        rt = routes[:-1].strip('[').split(', ')\n",
    "    else:\n",
    "        rt = routes.strip('][').split(', ')\n",
    "    rt = list(rt)\n",
    "    rt = [int(i) for i in rt]\n",
    "    return(rt)\n",
    "\n",
    "#define a function to ensure crossing routes\n",
    "def get_crossed_ct(osmid):\n",
    "    index = routes_id_dicts[osmid]\n",
    "    census_tract = cross_ct_dicts[index]\n",
    "    return(index, census_tract)\n",
    "\n",
    "def add_crossed_ct2dt(dt, index_check_lst):\n",
    "    census_t = []\n",
    "    osmids = list(dt['osmids'].values)\n",
    "    for osmid in osmids:\n",
    "        index, ct = get_crossed_ct(osmid)\n",
    "        if index not in index_check_lst:\n",
    "            index_check_lst.append(index)\n",
    "        census_t.append(ct)\n",
    "    dt['crossed_ct'] = census_t\n",
    "    return(index_check_lst)\n",
    "\n",
    "#the points are remapped after the shortest path algorithm\n",
    "#thus, in this case, we will use the remapped origin and destinated ct to ensure it also matches the crossed route results\n",
    "def find_ct(rt, missing_node, origin=True):\n",
    "    ori = 0\n",
    "    if origin==True:\n",
    "        for i in range(len(rt)):\n",
    "            if rt[i] in node_in_ct:\n",
    "                ori = node_in_ct[rt[i]]\n",
    "                break\n",
    "    else:\n",
    "        dest_index = -1\n",
    "        for i in range(len(rt)):\n",
    "            if rt[(-1-i)] in node_in_ct:\n",
    "                ori = node_in_ct[rt[(-1-i)]]\n",
    "                break\n",
    "    if ori == 0:\n",
    "        for osmid in rt:\n",
    "            if osmid not in missing_node:\n",
    "                missing_node.append(osmid)\n",
    "    return(ori, missing_node)\n",
    "\n",
    "def get_newod(osmid, missed_node):\n",
    "    rt = convert_routetype(osmid)\n",
    "    ori, missed_node = find_ct(rt, missed_node, True)\n",
    "    dest, missed_node = find_ct(rt, missed_node, False)\n",
    "    return(ori, dest, missed_node)\n",
    "\n",
    "def find_ctfromosmid(osmids):\n",
    "    route = convert_routetype(osmids)\n",
    "    crossed_ct = []\n",
    "    for i in route:\n",
    "        if i in node_in_ct:\n",
    "            if node_in_ct[i] not in crossed_ct:\n",
    "                crossed_ct.append(node_in_ct[i])\n",
    "    return(crossed_ct)\n",
    "\n",
    "#G_nodeswithct = pd.read_csv('G:/My Drive/2021/Bias/census_tract_process/nodes_in_cts.csv')\n",
    "#input_path = 'G:/My Drive/2021/Bias/census_tract_process/nodes_in_cts.csv', geoid_col ='GEOID10'\n",
    "def get_node_info(input_path, geoid_col, ct_index = False):\n",
    "    G_nodeswithct = pd.read_csv(input_path)\n",
    "    print(G_nodeswithct.head(3))\n",
    "    #print(np.unique(G_nodeswithct['GEOID10']))\n",
    "    print(np.unique(G_nodeswithct[geoid_col]))\n",
    "\n",
    "    #get points to census tract dict\n",
    "    osmids = list(G_nodeswithct['osmid'].values)\n",
    "    #geoids = list(G_nodeswithct['GEOID10'].values)\n",
    "    geoids = list(G_nodeswithct[geoid_col].values)\n",
    "\n",
    "    node_in_ct = {}\n",
    "    for i in range(len(osmids)):\n",
    "        if np.isnan(geoids[i]) == False:\n",
    "            node_in_ct[osmids[i]] = int(geoids[i])\n",
    "\n",
    "    #manual add some of the feature to census tract\n",
    "    if ct_index == True:\n",
    "        node_in_ct[53214121] = 53033007402\n",
    "        node_in_ct[53129446] = 53033007401\n",
    "        node_in_ct[53073260] = 53033007402\n",
    "        \n",
    "    return(G_nodeswithct, node_in_ct)\n",
    "\n",
    "#get nearest nodes\n",
    "def get_mapped_node(point_lat, point_lon, geoid_col):\n",
    "    node_loc =ox.distance.nearest_nodes(G, point_lon, point_lat)\n",
    "    try:\n",
    "        ct_id = G_nodeswithct[geoid_col][G_nodeswithct['osmid']==node_loc].values[0]\n",
    "        if np.isnan(ct_id) != True:\n",
    "            ct_id = int(ct_id)\n",
    "        else:\n",
    "            ct_id = 0\n",
    "    except:\n",
    "        ct_id = 0\n",
    "    return(ct_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba311ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_nodeswithct, node_in_ct = get_node_info('G:/My Drive/2021/Bias/census_tract_process/nodes_in_cts.csv', 'GEOID10', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205e8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_nodeswithct, node_in_ct = get_node_info('G:/My Drive/2021/Bias/census_block_data/nodes_in_land_use.csv', 'ZONEID')\n",
    "new_geoids = []\n",
    "for ids in latlon_dict:\n",
    "    latlon_pair = latlon_dict[ids]\n",
    "    for i in range(len(latlon_pair)):\n",
    "        #print(ids, i)\n",
    "        new_geoids.append(get_mapped_node(latlon_pair[i][0], latlon_pair[i][1], 'ZONEID'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #join routes to census tract\n",
    "uniq_routes = list(np.unique(route_file['osmids'].values))\n",
    "\n",
    "routes_id_dicts = {}\n",
    "cross_ct_dicts = {}\n",
    "for index in range(len(uniq_routes)):\n",
    "    #print(index)\n",
    "    routes_id_dicts[uniq_routes[index]] = index\n",
    "    cross_ct_dicts[index] = find_ctfromosmid(uniq_routes[index])\n",
    "    \n",
    "new_ori = []\n",
    "new_dest = []\n",
    "missed_node = []\n",
    "\n",
    "for route in route_file['osmids'].values:\n",
    "    newo, newd, missed_node = get_newod(route, missed_node)\n",
    "    new_ori.append(newo)\n",
    "    new_dest.append(newd)\n",
    "    \n",
    "route_file['new_ori'] = new_ori\n",
    "route_file['new_dest'] = new_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43199881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate od types\n",
    "#assign an index\n",
    "route_file['od_euqal'] = list((route_file['new_ori'] == route_file['new_dest']).values)\n",
    "#for the same od, check if the shortest path will cross the similar ods\n",
    "#to correct the current od matrice, we will need to know the origin, destination\n",
    "#and crossed time period\n",
    "#ods with same o and same d\n",
    "route_wsameod = route_file[route_file['od_euqal']==True]\n",
    "route_wosameod = route_file[route_file['od_euqal']==False]\n",
    "\n",
    "print('num data with same od',len(route_wsameod['od_euqal']))\n",
    "print('num data without same od',len(route_wosameod['od_euqal']))\n",
    "print('percentage of same od',len(route_wsameod['od_euqal'])/len(route_file['od_euqal']))\n",
    "index_check_lst = []\n",
    "index_check_lst = add_crossed_ct2dt(route_wsameod, index_check_lst)\n",
    "index_check_lst = add_crossed_ct2dt(route_wosameod, index_check_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for od within the same census tract,\n",
    "#check if there are other crossed ct\n",
    "ct_check = lambda x: 0 if (len(x) > 2) else 1\n",
    "route_wsameod['ct_check'] = route_wsameod['crossed_ct'].apply(ct_check).values\n",
    "\n",
    "#ct_check = lambda x: 0 if (len(x) > 3) else 1\n",
    "#route_wosameod['ct_check'] = route_wosameod['crossed_ct'].apply(ct_check).values\n",
    "#route_wosameod.head(3)\n",
    "    \n",
    "#route_wosameod[route_wosameod['ct_check']==0].head(10)\n",
    "route_wosameod['ct_check'] = 2\n",
    "\n",
    "#identify census tract not in the crossed ct\n",
    "geoids = list(np.unique(route_wosameod['new_ori']))\n",
    "get_nocrossed_ct = lambda x: list(set(x)^set(geoids))\n",
    "route_wosameod['no_crossed_ct'] = route_wosameod['crossed_ct'].apply(get_nocrossed_ct).values\n",
    "route_wsameod['no_crossed_ct'] = route_wsameod['crossed_ct'].apply(get_nocrossed_ct).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_file = pd.concat([route_wsameod, route_wosameod])\n",
    "#convert to csv\n",
    "cross_file.to_csv('crossed_routes.csv')\n",
    "\n",
    "#del(route_file)\n",
    "#del(route_wosameod)\n",
    "#del(route_wsameod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post data processing\n",
    "#made zero matrix \n",
    "#Functions\n",
    "#for the ods at the same location\n",
    "#the other location at the same time should be 0\n",
    "#time_diff <= 10 min\n",
    "\n",
    "#for the ods at different location\n",
    "#the other location which not belongs to the route or neatby od should be 0\n",
    "#time_diff <= 20 min\n",
    "\n",
    "#get 0 value\n",
    "\n",
    "#besides,if appears in one place already, the location will not appear in the other census tract\n",
    "def extract_od(dt, sameod = True):\n",
    "    if sameod == True:\n",
    "        test_dt = dt[(dt['od_euqal']==1) & (dt['time_diff']<=10)]\n",
    "    else:\n",
    "        test_dt = dt[(dt['od_euqal']==0) & (dt['time_diff']<=20)]\n",
    "    return(test_dt)\n",
    "\n",
    "def convert2stid(hr, minut_5, census):\n",
    "    return(str(census)+'_'+str(hr)+'_'+str(minut_5))\n",
    "\n",
    "def process_dt4convert(shr, ehr, smin, emin):\n",
    "    if ehr-shr<0:\n",
    "        print('data error')\n",
    "        return(None)\n",
    "    if ehr-shr == 0:\n",
    "        if emin-smin == 0:\n",
    "            return([shr], [smin])\n",
    "        elif emin-smin <0:\n",
    "            print('data error')\n",
    "            return(None)\n",
    "        else:\n",
    "            interval = int((emin-smin)/5)\n",
    "            min_lst = [smin]\n",
    "            hr_lst = [shr]\n",
    "            if interval > 1:\n",
    "                for i in range(interval):\n",
    "                    min_lst.append(int(smin+5*(i+1)))\n",
    "                    hr_lst.append(shr)\n",
    "            else:\n",
    "                min_lst.append(emin)\n",
    "                hr_lst.append(shr)\n",
    "            return(hr_lst, min_lst)\n",
    "    else:\n",
    "        if emin+60-smin == 0:\n",
    "            return([shr], [smin]) \n",
    "        else:\n",
    "            interval = int((emin+60-smin)/5)    \n",
    "            hr_lst = [shr]\n",
    "            min_lst = [smin]\n",
    "            if interval > 1:\n",
    "                for i in range(interval):\n",
    "                    if smin+(i+1)*5 >= 60:\n",
    "                        min_lst.append(smin+(i+1)*5-60)\n",
    "                        hr_lst.append(ehr)\n",
    "                    else:\n",
    "                        min_lst.append(smin+(i+1)*5)\n",
    "                        hr_lst.append(shr)\n",
    "            else:             \n",
    "                min_lst.append(emin)\n",
    "                hr_lst.append(ehr)\n",
    "            return(hr_lst, min_lst) \n",
    "        \n",
    "def add_zero_loc(shr, ehr, s5min, e5min, nocrossedcts, zero_loclst):\n",
    "    for i in range(len(shr)):\n",
    "        hr, minut = process_dt4convert(shr[i], ehr[i], s5min[i], e5min[i])\n",
    "        for j in nocrossedcts[i]:\n",
    "            for k in range(len(hr)):\n",
    "                loc_timeids = convert2stid(hr[k], minut[k], j)\n",
    "                if loc_timeids not in zero_loclst:\n",
    "                    zero_loclst.append(loc_timeids)\n",
    "    return(zero_loclst)\n",
    "\n",
    "def get_nocrossed_stids(dt, zero_loc_time):\n",
    "    dt = dt.sort_values(by=['newid'])\n",
    "    newids = np.unique(dt['newid'])\n",
    "    \n",
    "    for ids in newids:\n",
    "        if ids not in zero_loc_time:\n",
    "            zero_loc_time[ids] = []\n",
    "        #get start and end hr\n",
    "        s_hr = list(dt['s_hr'][dt['newid']==ids].values)\n",
    "        e_hr = list(dt['e_hr'][dt['newid']==ids].values)\n",
    "        s_5min = list(dt['s_minut_5'][dt['newid']==ids].values)\n",
    "        e_5min = list(dt['e_minut_5'][dt['newid']==ids].values)\n",
    "        no_crossed_cts = list(dt['no_crossedcts'][dt['newid']==ids].values)\n",
    "        \n",
    "        zero_loc_time[ids] = add_zero_loc(s_hr, e_hr, s_5min, e_5min, no_crossed_cts, zero_loc_time[ids])\n",
    "    return(zero_loc_time)\n",
    "\n",
    "def getzero_info(dt):\n",
    "    zero_loc_time = {}\n",
    "    dt1 = extract_od(dt, sameod = True)\n",
    "    dt2 = extract_od(dt, sameod = False)\n",
    "    zero_loc_time = get_nocrossed_stids(dt1, zero_loc_time)\n",
    "    zero_loc_time = get_nocrossed_stids(dt2, zero_loc_time)\n",
    "    return(zero_loc_time)\n",
    "\n",
    "def get_revised_matrix(pred_matrix, zero_loc_time):\n",
    "    for ids in zero_loc_time:\n",
    "        for loct_time in zero_loc_time[ids]:\n",
    "            pred_matrix[newids_map[ids]][locts_map[loct_time]] = 0\n",
    "    return(pred_matrix)\n",
    "\n",
    "def convert_missingids(time_lst, ids, time_spatial_ids, Seattle_latlon):\n",
    "    for t in time_lst:\n",
    "        hr = (Seattle_latlon['hr'][(Seattle_latlon['newid']==ids) & (Seattle_latlon['timestamp']==t)]).values[0]\n",
    "        minut = (Seattle_latlon['minut_5'][(Seattle_latlon['newid']==ids) & (Seattle_latlon['timestamp']==t)]).values[0]\n",
    "        geoid = (Seattle_latlon['geoid'][(Seattle_latlon['newid']==ids) & (Seattle_latlon['timestamp']==t)]).values[0]\n",
    "        newgeo = (Seattle_latlon['new_geos'][(Seattle_latlon['newid']==ids) & (Seattle_latlon['timestamp']==t)]).values[0]\n",
    "        geos = list(np.unique([geoid, newgeo]))\n",
    "        if 0 in geos:\n",
    "            geos.remove(0)\n",
    "        #get no observed census_t\n",
    "        diff_ct = list(set(census_t) ^ set(geos))\n",
    "        for geoid in diff_ct:\n",
    "            time_spatial_id = convert2stid(hr, minut, geoid)\n",
    "            if time_spatial_id not in time_spatial_ids:\n",
    "                time_spatial_ids.append(time_spatial_id)\n",
    "    return(time_spatial_ids)\n",
    "\n",
    "latlon_path = 'G:/My Drive/2021/Bias/census_tract_process/'\n",
    "Seattle_latlon = pd.read_csv(latlon_path+'Seattle_latlon.csv', index_col=0)\n",
    "\n",
    "#convert data with estimated shortest path info\n",
    "#file_path = 'G:/My Drive/2021/Bias/census_tract_process/crossed_routes.csv'\n",
    "#crossed_rt = pd.read_csv(file_path)\n",
    "def process_crossed_file(crossed_rt):\n",
    "    get_hr = lambda x: datetime.fromtimestamp(x).hour\n",
    "    get_minute = lambda x: datetime.fromtimestamp(x).minute\n",
    "    crossed_rt['s_hr'] = crossed_rt['start_time'].apply(get_hr).values\n",
    "    crossed_rt['e_hr'] = crossed_rt['end_time'].apply(get_hr).values\n",
    "    crossed_rt['s_min'] = crossed_rt['start_time'].apply(get_minute).values\n",
    "    crossed_rt['e_min'] = crossed_rt['end_time'].apply(get_minute).values\n",
    "\n",
    "    to5min = lambda x: int(x/5)*5\n",
    "    crossed_rt['s_minut_5'] = crossed_rt['s_min'].apply(to5min)\n",
    "    crossed_rt['e_minut_5'] = crossed_rt['e_min'].apply(to5min)\n",
    "\n",
    "    get_timeloct_id = lambda x: str(x)+'_'\n",
    "    remove_last_symbol = lambda x: x[:-1]\n",
    "    crossed_rt['ori_st'] = (crossed_rt['new_ori'].apply(get_timeloct_id)+\\\n",
    "    crossed_rt['s_hr'].apply(get_timeloct_id)+\\\n",
    "    crossed_rt['s_minut_5'].apply(get_timeloct_id)).apply(remove_last_symbol)\n",
    "\n",
    "    crossed_rt['dest_st'] = (crossed_rt['new_dest'].apply(get_timeloct_id)+\\\n",
    "    crossed_rt['e_hr'].apply(get_timeloct_id)+\\\n",
    "    crossed_rt['e_minut_5'].apply(get_timeloct_id)).apply(remove_last_symbol)\n",
    "\n",
    "    return(crossed_rt)\n",
    "\n",
    "\n",
    "crossed_rt = process_crossed_file(cross_file)\n",
    "### sparse svd\n",
    "#predsvd_rmse_s, lfsvd_rmse_s, rmse_svd_s = get_best_svd(test_matrice_scaled_reverse, False)\n",
    "#predsvd_rmse, lfsvd_rmse, rmse_svd = get_best_svd(test_matrice_reverse, False)\n",
    "\n",
    "#nmf_rmse_dict, nmf_zero_score, nmf_mae_dict, nmf_rmse_w0, nmf_mae_w0 = get_best_NMFwzero(test_matrice_reverse, Zero_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa663b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_rt = crossed_rt[crossed_rt.columns[5:]]\n",
    "crossed_rt.columns\n",
    "\n",
    "def read_dict(filepath):\n",
    "    with open(filepath) as f:\n",
    "        data = f.read()\n",
    "    dict_data = ast.literal_eval(data)\n",
    "    return(dict_data)\n",
    "\n",
    "def convert_dict(index_dict):\n",
    "    convertdict = {}\n",
    "    for keys in index_dict:\n",
    "        convertdict[index_dict[keys]] = keys\n",
    "    return(convertdict)\n",
    "\n",
    "\n",
    "row_index = read_dict('row_index.txt')\n",
    "col_index = read_dict('col_index.txt')\n",
    "\n",
    "user_index = convert_dict(row_index)\n",
    "item_index = convert_dict(col_index)\n",
    "\n",
    "crossed_rt.to_csv('crossed_rt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check time difference change for each individual\n",
    "#this part needs to be processed in the algorithm code(either in NMF/AE)\n",
    "'''\n",
    "newids_map = row_index\n",
    "newids = list(newids_map.keys())\n",
    "a = 0\n",
    "index = 0\n",
    "for ids in newids:\n",
    "    if a <= index:\n",
    "        test_dt = crossed_rt[['time_diff', 's_hr','e_hr','od_euqal']][crossed_rt['newid'] == ids]\n",
    "        test_dt = test_dt.sort_values(by=['time_diff'])\n",
    "        plt.figure(a)\n",
    "        x = list(test_dt['s_hr'].values)\n",
    "        y = list(test_dt['time_diff'].values)\n",
    "        plt.plot(x,y,'o')\n",
    "        plt.title('time difference between each time segement for ids = '+str(ids))\n",
    "        a += 1\n",
    "        \n",
    "test_dt = crossed_rt[['s_hr', 'time_diff']][crossed_rt['od_euqal']==0].sort_values(by=['s_hr', 'time_diff'])\n",
    "plt.figure(0)\n",
    "plt.plot(list(test_dt['s_hr'].values), list(test_dt['time_diff'].values),'o')\n",
    "plt.title('time difference across day')\n",
    "\n",
    "#keep previous census tract\n",
    "ori_geo = crossed_rt['ori_geo'].values\n",
    "dest_geo = crossed_rt['dest_geo'].values\n",
    "no_crossed_ct = crossed_rt['no_crossed_ct'].values\n",
    "no_crossed_cts = []\n",
    "for i in range(len(ori_geo)):\n",
    "    no_crossed = no_crossed_ct[i].strip('[]').split(', ')\n",
    "    if str(ori_geo[i]) in no_crossed:\n",
    "        no_crossed.remove(str(ori_geo[i]))\n",
    "    if str(dest_geo[i]) in no_crossed:\n",
    "        no_crossed.remove(str(dest_geo[i]))\n",
    "    no_crossed_cts.append(no_crossed)\n",
    "\n",
    "del(ori_geo)\n",
    "del(dest_geo)\n",
    "del(no_crossed_ct)\n",
    "\n",
    "crossed_rt['no_crossedcts'] = no_crossed_cts\n",
    "crossed_rt = crossed_rt.sort_values(by=['newid','time_diff','s_hr'])\n",
    "\n",
    "#we found missing valeues after we processing the data\n",
    "#latlon_path = 'G:/My Drive/2021/Bias/data-processing/'\n",
    "#Seattle_latlon = pd.read_csv('Seattle_latlon.csv', index_col=0)\n",
    "\n",
    "selected_colname = list(latlon_file.columns)\n",
    "#selected_colname.remove('Unnamed: 0.1')\n",
    "\n",
    "latlon_file['minut_5'] = (latlon_file['minut'].apply(to5min)).values\n",
    "#through the checking, it is found there are individuals missing after the shortest path algorithm\n",
    "ids_in_dt = list(np.unique(crossed_rt['newid']))\n",
    "diff = set(newids)^set(ids_in_dt)\n",
    "\n",
    "print('# of ids with shortest path: ', len(ids_in_dt))\n",
    "print('# of identified ids: ', len(newids_map.keys()))\n",
    "print('# of missing ids without shortest path: ', len(diff))\n",
    "\n",
    "#check the missed data in detail\n",
    "\n",
    "crossed_rt = crossed_rt.sort_values(by=['newid','start_time'])\n",
    "crossed_rt.reset_index(inplace=True)\n",
    "crossed_rt.head(3)\n",
    "\n",
    "newids = np.unique(crossed_rt['newid'])\n",
    "kept_data_start = {}\n",
    "kept_data_end = {}\n",
    "for ids in newids:\n",
    "    kept_data_start[ids] = list(crossed_rt['start_time'][crossed_rt['newid']==ids].values)\n",
    "    kept_data_end[ids] = list(crossed_rt['end_time'][crossed_rt['newid']==ids].values)\n",
    "    \n",
    "#get missing trips\n",
    "#missing ids with missing data\n",
    "newids = np.unique(crossed_rt['newid'])\n",
    "newids_all = list(newids_map.keys())\n",
    "missing_ids = {}\n",
    "for ids in newids_all:\n",
    "    missing_ids[ids] = []\n",
    "    time_data = list(latlon_file['timestamp'][latlon_file['newid']==ids].values)\n",
    "    if ids not in newids:\n",
    "        missing_ids[ids] = time_data\n",
    "    else:    \n",
    "        for time in time_data:\n",
    "            if time not in kept_data_end[ids] and time not in kept_data_start[ids]:\n",
    "                missing_ids[ids].append(time)\n",
    "                \n",
    "missing_pairs = {}\n",
    "for ids in newids:\n",
    "    missing_pairs[ids] = {'o':[],'d':[]}\n",
    "    cur_odpairs = crossed_rt[['start_time', 'end_time']][crossed_rt['newid']==ids].values\n",
    "    time_data = list(latlon_file['timestamp'][latlon_file['newid']==ids].values)\n",
    "    for i in range(len(time_data)):\n",
    "        if i < len(time_data)-1:\n",
    "            od_pairs = []\n",
    "            od_pairs.append(time_data[i]) \n",
    "            od_pairs.append(time_data[i+1])\n",
    "            if od_pairs not in cur_odpairs:\n",
    "                missing_pairs[ids]['o'].append(od_pairs[0])\n",
    "                missing_pairs[ids]['d'].append(od_pairs[1]) \n",
    "                \n",
    "                \n",
    "print('worst data accuracy: ', np.max(latlon_file['accuracy']))\n",
    "\n",
    "census_t = list(np.unique(latlon_file['GEOID10']))\n",
    "\n",
    "zero_loc_time = {}\n",
    "for ids in missing_ids:\n",
    "    if ids not in zero_loc_time:\n",
    "        zero_loc_time[ids] = []\n",
    "    zero_loc_time[ids] = convert_missingids(missing_ids[ids], ids, zero_loc_time[ids], latlon_file)\n",
    "    \n",
    "# create the zero index matrix\n",
    "Zero_index = np.zeros(test_matrice_reverse[:, 1:].shape)\n",
    "for row in zero_loc_time:\n",
    "    for col in zero_loc_time[row]:\n",
    "        Zero_index[newids_map[row]][locts_map[col]] = 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous data process for census tract\n",
    "'''\n",
    "#get neighborhood census tract info\n",
    "os.chdir('G:/My Drive/2021/Bias/census_tract_process')\n",
    "pathfiale = 'G:/My Drive/2021/Bias/census_tract_process/'\n",
    "Seattle_ct = pd.read_csv(pathfile+'Seattle_ct.csv')\n",
    "Seattle_ct.columns\n",
    "Seattle_ct = Seattle_ct[['GEOID10','Nb_interse', 'nb_count', 'nb_touch', 'nb_bf_5', 'nb_bf10', \\\n",
    "                         'nb_bf100','nb_bf1mi','Area', 'Distance m','Distance_1', 'field_1', 'index']]\n",
    "\n",
    "#check time difference change for each individual\n",
    "newids_map = row_index\n",
    "newids = list(newids_map.keys())\n",
    "a = 0\n",
    "index = 0\n",
    "for ids in newids:\n",
    "    if a <= index:\n",
    "        test_dt = crossed_rt[['time_diff', 'travel_time','s_hr','e_hr','od_euqal']][crossed_rt['newid'] == ids]\n",
    "        test_dt = test_dt.sort_values(by=['time_diff'])\n",
    "        plt.figure(a)\n",
    "        x = list(test_dt['s_hr'].values)\n",
    "        y = list(test_dt['time_diff'].values)\n",
    "        plt.plot(x,y,'o')\n",
    "        plt.title('time difference between each time segement for ids = '+str(ids))\n",
    "        a += 1\n",
    "        \n",
    "test_dt = crossed_rt[['s_hr', 'time_diff', 'travel_time']][crossed_rt['od_euqal']==0].sort_values(by=['s_hr', 'time_diff'])\n",
    "plt.figure(0)\n",
    "plt.plot(list(test_dt['s_hr'].values), list(test_dt['time_diff'].values),'o')\n",
    "plt.title('time difference across day')\n",
    "plt.figure(1)\n",
    "plt.plot(list(test_dt['s_hr'].values), list(test_dt['travel_time'].values),'ro')\n",
    "plt.title('travel time across day (estimated based on shortest path)')\n",
    "\n",
    "#keep previous census tract\n",
    "ori_geo = crossed_rt['ori_geo'].values\n",
    "dest_geo = crossed_rt['dest_geo'].values\n",
    "no_crossed_ct = crossed_rt['no_crossed_ct'].values\n",
    "no_crossed_cts = []\n",
    "for i in range(len(ori_geo)):\n",
    "    no_crossed = no_crossed_ct[i].strip('[]').split(', ')\n",
    "    if str(ori_geo[i]) in no_crossed:\n",
    "        no_crossed.remove(str(ori_geo[i]))\n",
    "    if str(dest_geo[i]) in no_crossed:\n",
    "        no_crossed.remove(str(dest_geo[i]))\n",
    "    no_crossed_cts.append(no_crossed)\n",
    "\n",
    "del(ori_geo)\n",
    "del(dest_geo)\n",
    "del(no_crossed_ct)\n",
    "\n",
    "crossed_rt['no_crossedcts'] = no_crossed_cts\n",
    "crossed_rt = crossed_rt.sort_values(by=['newid','time_diff','s_hr'])\n",
    "\n",
    "\n",
    "#we found missing valeues after we processing the data\n",
    "#latlon_path = 'G:/My Drive/2021/Bias/data-processing/'\n",
    "Seattle_latlon = pd.read_csv('Seattle_latlon.csv', index_col=0)\n",
    "\n",
    "selected_colname = list(Seattle_latlon.columns)\n",
    "selected_colname.remove('Unnamed: 0.1')\n",
    "Seattle_latlon = Seattle_latlon[selected_colname]\n",
    "Seattle_latlon['minut_5'] = (Seattle_latlon['minut'].apply(to5min)).values\n",
    "#through the checking, it is found there are individuals missing after the shortest path algorithm\n",
    "ids_in_dt = list(np.unique(crossed_rt['newid']))\n",
    "diff = set(newids)^set(ids_in_dt)\n",
    "\n",
    "print('# of ids with shortest path: ', len(ids_in_dt))\n",
    "print('# of identified ids: ', len(newids_map.keys()))\n",
    "print('# of missing ids without shortest path: ', len(diff))\n",
    "\n",
    "#check the missed data in detail\n",
    "\n",
    "crossed_rt = crossed_rt.sort_values(by=['newid','start_time'])\n",
    "crossed_rt.reset_index(inplace=True)\n",
    "crossed_rt.head(3)\n",
    "\n",
    "newids = np.unique(crossed_rt['newid'])\n",
    "kept_data_start = {}\n",
    "kept_data_end = {}\n",
    "for ids in newids:\n",
    "    kept_data_start[ids] = list(crossed_rt['start_time'][crossed_rt['newid']==ids].values)\n",
    "    kept_data_end[ids] = list(crossed_rt['end_time'][crossed_rt['newid']==ids].values)\n",
    "    \n",
    "#get missing trips\n",
    "#missing ids with missing data\n",
    "newids = np.unique(crossed_rt['newid'])\n",
    "newids_all = list(newids_map.keys())\n",
    "missing_ids = {}\n",
    "for ids in newids_all:\n",
    "    missing_ids[ids] = []\n",
    "    time_data = list(Seattle_latlon['timestamp'][Seattle_latlon['newid']==ids].values)\n",
    "    if ids not in newids:\n",
    "        missing_ids[ids] = time_data\n",
    "    else:    \n",
    "        for time in time_data:\n",
    "            if time not in kept_data_end[ids] and time not in kept_data_start[ids]:\n",
    "                missing_ids[ids].append(time)\n",
    "                \n",
    "missing_pairs = {}\n",
    "for ids in newids:\n",
    "    missing_pairs[ids] = {'o':[],'d':[]}\n",
    "    cur_odpairs = crossed_rt[['start_time', 'end_time']][crossed_rt['newid']==ids].values\n",
    "    time_data = list(Seattle_latlon['timestamp'][Seattle_latlon['newid']==ids].values)\n",
    "    for i in range(len(time_data)):\n",
    "        if i < len(time_data)-1:\n",
    "            od_pairs = []\n",
    "            od_pairs.append(time_data[i]) \n",
    "            od_pairs.append(time_data[i+1])\n",
    "            if od_pairs not in cur_odpairs:\n",
    "                missing_pairs[ids]['o'].append(od_pairs[0])\n",
    "                missing_pairs[ids]['d'].append(od_pairs[1]) \n",
    "                \n",
    "                \n",
    "print('worst data accuracy: ', np.max(Seattle_latlon['accuracy']))\n",
    "\n",
    "census_t = list(np.unique(Seattle_latlon['GEOID10']))\n",
    "\n",
    "zero_loc_time = {}\n",
    "for ids in missing_ids:\n",
    "    if ids not in zero_loc_time:\n",
    "        zero_loc_time[ids] = []\n",
    "    zero_loc_time[ids] = convert_missingids(missing_ids[ids], ids, zero_loc_time[ids])\n",
    "    \n",
    "# create the zero index matrix\n",
    "Zero_index = np.zeros(test_matrice_reverse[:, 1:].shape)\n",
    "for row in zero_loc_time:\n",
    "    for col in zero_loc_time[row]:\n",
    "        Zero_index[newids_map[row]][locts_map[col]] = 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#previous code for directly route join\n",
    "#there will be route info missings\n",
    "cross_routes = pd.read_csv('crossed_routes.csv')\n",
    "cross_routes.head(3)\n",
    "\n",
    "#process the nan values\n",
    "geoid = []\n",
    "for i in cross_routes['GEOID10'].values:\n",
    "    if np.isnan(i)==True:\n",
    "        geoid.append(0)\n",
    "    else:\n",
    "        geoid.append(int(i))\n",
    "        \n",
    "missing_rt = check_missing_routes(route_file, cross_routes)\n",
    "missing_routes = []\n",
    "for routes in missing_rt:\n",
    "    rt = convert_routetype(routes)\n",
    "    missing_routes.append(rt)\n",
    "\n",
    "#del(missing_rt)\n",
    "\n",
    "est_rt = np.unique(route_file['osmids'])\n",
    "join_rt = np.unique(cross_routes['osmids'])\n",
    "a = 0\n",
    "missed_index = []\n",
    "for i in range(len(join_rt)):\n",
    "    if join_rt[i] not in est_rt:\n",
    "        missed_index.append(i)\n",
    "        a += 1\n",
    "\n",
    "missing_details = np.unique(missing_routes)\n",
    "\n",
    "#plot missing details\n",
    "for i in missing_routes:\n",
    "    if a <= 15:\n",
    "        print(i)\n",
    "        route_plot(convert_routetype(i))\n",
    "        a += 1\n",
    "\n",
    "cross_routes['geoid'] = geoid\n",
    "\n",
    "#check routes\n",
    "i = 0 \n",
    "for rt_no_geo in cross_routes['osmids'][cross_routes['geoid']==0].values:\n",
    "    if i <= 10:\n",
    "        rt = convert_routetype(rt_no_geo)\n",
    "        route_plot(rt)\n",
    "        i +=1\n",
    "\n",
    "#replace the nan value based on the plot checking\n",
    "geoid = []\n",
    "for i in cross_routes['GEOID10'].values:\n",
    "    if np.isnan(i)==True:\n",
    "        geoid.append(53033007402)\n",
    "    else:\n",
    "        geoid.append(int(i))\n",
    "\n",
    "cross_routes['geoid'] = geoid\n",
    "\n",
    "cross_routes = cross_routes.sort_values(by=['index'])\n",
    "cross_routes = cross_routes.reset_index(inplace=True)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
