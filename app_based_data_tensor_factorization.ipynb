{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\yiran\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\yiran\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yiran\\anaconda3\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\yiran\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorly\n",
    "\n",
    "#imported packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker\n",
    "import math\n",
    "from scipy import stats\n",
    "import csv\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "import time\n",
    "\n",
    "# LDA for clusterring\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defined functions\n",
    "#data column methods\n",
    "\n",
    "#convert data to array\n",
    "def getseqid(targetval):\n",
    "    #convert taz idx\n",
    "    targetidx = {}\n",
    "    idxtarget = {}\n",
    "    idx = 0\n",
    "    for i in targetval:\n",
    "        targetidx[i] = idx        \n",
    "        idxtarget[idx] = i\n",
    "        idx += 1\n",
    "    return(targetidx, idxtarget)\n",
    "\n",
    "def data2arr(datapath, binaryidx=True, to2d=True):\n",
    "    taz2idx = lambda x: tazidx[x]\n",
    "    str2time = lambda x: int(int(x.split(':')[0])*12 + int(x.split(':')[1])/5)\n",
    "    timelen = 24*12 #with 5-min interval\n",
    "    newid2idx = lambda x: newididx[x]\n",
    "    \n",
    "    datapath = raw_dt_path[0]\n",
    "    dt = pd.read_csv(datapath)\n",
    "    \n",
    "    #convert time and location\n",
    "    tazidx, idxtaz = getseqid(np.unique(dt['taz']))\n",
    "    dt['tazidx'] = dt['taz'].apply(taz2idx)\n",
    "    dt['timeidx'] = dt['time'].apply(str2time)\n",
    "\n",
    "    newididx, idxnewid = getseqid(np.unique(dt['newid']))\n",
    "    dt['newididx'] = dt['newid'].apply(newid2idx)\n",
    "    \n",
    "    if to2d == True:\n",
    "        dt['ltidx'] = dt['tazidx']*timelen+dt['timeidx']\n",
    "        nrow = len(newididx)\n",
    "        ncol = timelen * len(tazidx)\n",
    "\n",
    "        arr = np.zeros((nrow, ncol))\n",
    "        dt = dt[['newididx','ltidx', 'sum']].to_numpy()\n",
    "\n",
    "        #assign binary index\n",
    "        for i, j, count in dt:\n",
    "            if binaryidx == True:\n",
    "                arr[i, j] = 1\n",
    "            else:\n",
    "                arr[i, j] = count\n",
    "        return(arr)\n",
    "    else:\n",
    "        nrow = len(newididx)\n",
    "        ntaz = len(tazidx)\n",
    "        ntime = timelen\n",
    "        \n",
    "        arr = np.zeros((nrow, ntaz, ntime))\n",
    "        dt = dt[['newididx', 'tazidx', 'timeidx', 'sum']].to_numpy()\n",
    "        \n",
    "        for i, j, k, count in dt:\n",
    "            if binaryidx == True:\n",
    "                arr[i, j, k] = 1\n",
    "            else:\n",
    "                arr[i, j, k] = sum\n",
    "        return(arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the fcd data\n",
    "path = \"G:/My Drive/2021/Bias/sumo_simulation/\"\n",
    "#os.listdir()\n",
    "\n",
    "raw_dt_path = []\n",
    "for i in os.listdir(path):\n",
    "    if len(i) == 12 and '2017' in i:\n",
    "        raw_dt_path.append(path+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastdtw\n",
    "#clustering\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary time series data\n",
    "# Replace this with your actual data\n",
    "# Calculate pairwise DTW distances\n",
    "def get_dtw_dist(arr, return_mx):\n",
    "    \n",
    "    num_samples = arr.shape[0]\n",
    "    dtw_distances = np.zeros((num_samples, num_samples))\n",
    "    #print(dtw_distances.shape)\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i, num_samples):\n",
    "            distance, _ = fastdtw(arr[i], arr[j], dist=euclidean)\n",
    "            dtw_distances[i, j] = distance\n",
    "            dtw_distances[j, i] = distance\n",
    "\n",
    "    # Normalize DTW distances (optional but can help with K-Means)\n",
    "    max_distance = np.max(dtw_distances)\n",
    "    normalized_dtw_distances = dtw_distances / max_distance\n",
    "    if return_mx == True:\n",
    "        return(dtw_distances, normalized_dtw_distances)\n",
    "    else:\n",
    "        return(normalized_dtw_distances)\n",
    "\n",
    "def conduct_cluster(normalized_dtw_distances, num_clusters):\n",
    "    # Apply K-Means clustering\n",
    "    #num_clusters = 3  # Adjust as needed\n",
    "    #normalized_dtw_distances = get_dtw_dist(arr, return_mx)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(normalized_dtw_distances)\n",
    "    # Print cluster labels\n",
    "    #print(\"Cluster Labels:\")\n",
    "    #print(cluster_labels)\n",
    "    db_index = davies_bouldin_score(normalized_dtw_distances, cluster_labels)\n",
    "    return(db_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13709, 13709)\n"
     ]
    }
   ],
   "source": [
    "#check the daily pattern across all the taz\n",
    "cluster_num = np.arange(2, 11)\n",
    "arr_test = data2arr(raw_dt_path[0])\n",
    "\n",
    "db_score = []\n",
    "dtw_mx, normalized_dtw = get_dtw_dist(arr_test, True)\n",
    "for num_c in cluster_num:\n",
    "    db_score.append(normalized_dtw, num_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the daily pattern for each individual \n",
    "arr_test = data2arr(raw_dt_path[0], to2d=False)\n",
    "\n",
    "\n",
    "db_scroe_test = []\n",
    "for i in arr_test.shape[0]:\n",
    "    dtw_mx_, normalized_dtw_ = get_dtw_dist(arr_test[i], True)\n",
    "    for num_c in cluster_num:\n",
    "        db_score_test.append(normalized_dtw_, num_c)\n",
    "    plt.plot(db_score_test)\n",
    "    plt.title(f'individual idx = {i}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davies-Bouldin Index: 2.53114765553973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_index = davies_bouldin_score(normalized_dtw_distances, cluster_labels)\n",
    "\n",
    "print(\"Davies-Bouldin Index:\", db_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13709, 167, 288)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_reshaped = arr.reshape(arr.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '1.000*\"0.0\" + 0.000*\"1.0\"')\n",
      "(1, '0.999*\"0.0\" + 0.001*\"1.0\"')\n",
      "(2, '1.000*\"0.0\" + 0.000*\"1.0\"')\n",
      "(3, '1.000*\"0.0\" + 0.000*\"1.0\"')\n",
      "(4, '1.000*\"0.0\" + 0.000*\"1.0\"')\n"
     ]
    }
   ],
   "source": [
    "#arr_reshaped = arr.reshape(arr.shape[0], -1)\n",
    "\n",
    "# Convert the binary data into a document-term matrix\n",
    "documents = [list(map(str, row)) for row in arr_reshaped]\n",
    "\n",
    "# Create a dictionary and a corpus for Gensim\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Apply LDA-like topic modeling\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "\n",
    "# Print the discovered topics\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starttime = time.time()\n",
    "\n",
    "rank_individuals = 10\n",
    "rank_locations = 10\n",
    "rank_times = 12\n",
    "\n",
    "core, factors = tucker(arr, rank=[rank_individuals, rank_locations, rank_times])\n",
    "endtime = time.time()\n",
    "print(f'running time {endtime-starttime}')\n",
    "\n",
    "reconstructed_data = tl.tucker_to_tensor((core, factors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
